{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import psutil\n",
    "import torch.optim as optim\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    process = psutil.Process()\n",
    "    return process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable parameters in the model\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_embeddings(model, train_loader, device, max_samples=500):\n",
    "    \"\"\"Evaluate embedding quality using silhouette score\"\"\"\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    spatial_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        sample_count = 0\n",
    "        for batch_data in train_loader:\n",
    "            if sample_count >= max_samples:\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                anchor = batch_data[\"anchor_image\"].to(device)\n",
    "                anchor_emb = model(anchor).cpu().numpy()\n",
    "                print(\"Processing batch with shape:\", anchor_emb.shape)\n",
    "\n",
    "                # Check for NaN or infinite values\n",
    "                if np.any(np.isnan(anchor_emb)) or np.any(np.isinf(anchor_emb)):\n",
    "                    print(\"Warning: NaN/Inf detected in embeddings, skipping batch\")\n",
    "                    continue\n",
    "\n",
    "                embeddings.append(anchor_emb)\n",
    "\n",
    "                # Create spatial pseudo-labels based on coordinates if available\n",
    "                # If no coordinates, create labels based on batch position as approximation\n",
    "                if \"coordinates\" in batch_data:\n",
    "                    coords = batch_data[\"coordinates\"].numpy()\n",
    "                    # Discretize coordinates into spatial bins for clustering\n",
    "                    lat_bins = np.digitize(\n",
    "                        coords[:, 0],\n",
    "                        bins=np.linspace(coords[:, 0].min(), coords[:, 0].max(), 10),\n",
    "                    )\n",
    "                    lon_bins = np.digitize(\n",
    "                        coords[:, 1],\n",
    "                        bins=np.linspace(coords[:, 1].min(), coords[:, 1].max(), 10),\n",
    "                    )\n",
    "                    labels = lat_bins * 10 + lon_bins\n",
    "                else:\n",
    "                    # Fallback: use simple sequential labeling\n",
    "                    labels = np.full(\n",
    "                        anchor_emb.shape[0], len(embeddings) % 5\n",
    "                    )  # Create 5 clusters\n",
    "\n",
    "                spatial_labels.append(labels)\n",
    "                sample_count += anchor_emb.shape[0]\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error processing batch in embedding evaluation: {e}\")\n",
    "                continue\n",
    "\n",
    "    if len(embeddings) < 2:\n",
    "        print(\"Warning: Not enough valid embeddings for silhouette score\")\n",
    "        return 0.0  # Not enough data for silhouette score\n",
    "\n",
    "    try:\n",
    "        # Concatenate all embeddings and labels\n",
    "        all_embeddings = np.vstack(embeddings)\n",
    "        all_labels = np.concatenate(spatial_labels)\n",
    "\n",
    "        # Final check for NaN values\n",
    "        if np.any(np.isnan(all_embeddings)) or np.any(np.isinf(all_embeddings)):\n",
    "            print(\"Warning: NaN/Inf found in concatenated embeddings\")\n",
    "            return 0.0\n",
    "\n",
    "        # If we don't have real spatial labels, use KMeans clustering\n",
    "        if \"coordinates\" not in batch_data:\n",
    "            n_clusters = min(\n",
    "                5, max(2, len(np.unique(all_labels)))\n",
    "            )  # Ensure 2-5 clusters\n",
    "            if n_clusters > 1 and len(all_embeddings) >= n_clusters:\n",
    "                try:\n",
    "                    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "                    cluster_labels = kmeans.fit_predict(all_embeddings)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: KMeans clustering failed: {e}\")\n",
    "                    return 0.0\n",
    "            else:\n",
    "                return 0.0\n",
    "        else:\n",
    "            cluster_labels = all_labels\n",
    "\n",
    "        # Calculate silhouette score\n",
    "        if len(np.unique(cluster_labels)) > 1 and len(all_embeddings) > 1:\n",
    "            try:\n",
    "                # Ensure we have enough samples per cluster\n",
    "                unique_labels, counts = np.unique(cluster_labels, return_counts=True)\n",
    "                if np.all(counts >= 1) and len(unique_labels) >= 2:\n",
    "                    silhouette_avg = silhouette_score(all_embeddings, cluster_labels)\n",
    "                    return float(silhouette_avg)\n",
    "                else:\n",
    "                    return 0.0\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Silhouette score calculation failed: {e}\")\n",
    "                return 0.0\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error in embedding evaluation: {e}\")\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision.models import (\n",
    "    ConvNeXt_Small_Weights,\n",
    "    EfficientNet_B0_Weights,\n",
    "    EfficientNet_V2_M_Weights,\n",
    "    EfficientNet_V2_S_Weights,\n",
    "    MobileNet_V3_Large_Weights,\n",
    "    MobileNet_V3_Small_Weights,\n",
    "    ResNet50_Weights,\n",
    "    Swin_S_Weights,\n",
    ")\n",
    "\n",
    "\n",
    "class Loc2VecModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels: int = 3,\n",
    "        embedding_dim: int = 16,\n",
    "        dropout_rate: float = 0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, padding=0, bias=True),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=0, bias=True),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, 3, padding=0, bias=True),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 64, 3, padding=0, bias=True),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, 3, padding=0, bias=True),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(128, 64, 1, padding=0, bias=True),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024, 64, bias=True),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(64, embedding_dim, bias=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class SoftmaxTripletLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Triplet Soft-max ratio loss (Ailon et al.) with optional SoftPN variant.\n",
    "    Minimises  MSE( (d_plus, d_minus), (0, 1) ).\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    softpn : bool\n",
    "        If True, use SoftPN (replace Δ(a,n) by min(Δ(a,n), Δ(p,n))).\n",
    "    squared : bool\n",
    "        If True, use squared Euclidean distance; else plain L2.\n",
    "    reduction : str\n",
    "        'mean' | 'sum' | 'none'   (mirrors PyTorch's reduction semantics)\n",
    "    eps : float\n",
    "        Numerical stabiliser added to denominator.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        softpn: bool = False,\n",
    "        squared: bool = True,\n",
    "        reduction: str = \"mean\",\n",
    "        eps: float = 1e-8,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if reduction not in (\"mean\", \"sum\", \"none\"):\n",
    "            raise ValueError(\"reduction must be 'mean', 'sum' or 'none'\")\n",
    "        self.softpn = softpn\n",
    "        self.squared = squared\n",
    "        self.reduction = reduction\n",
    "        self.eps = eps\n",
    "\n",
    "    @staticmethod\n",
    "    def _l2(a: torch.Tensor, b: torch.Tensor, squared: bool) -> torch.Tensor:\n",
    "        out = (a - b).pow(2).sum(dim=1)\n",
    "        return out if squared else out.clamp_min(1e-12).sqrt()\n",
    "\n",
    "    def forward(\n",
    "        self, anchor: torch.Tensor, positive: torch.Tensor, negative: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        anchor, positive, negative  : shape (B, embedding_dim)\n",
    "        returns scalar loss (or per-sample loss if reduction='none')\n",
    "        \"\"\"\n",
    "        delta_ap = self._l2(anchor, positive, self.squared)\n",
    "        delta_an = self._l2(anchor, negative, self.squared)\n",
    "\n",
    "        if self.softpn:\n",
    "            delta_pn = self._l2(positive, negative, self.squared)\n",
    "            delta_neg = torch.min(delta_an, delta_pn)\n",
    "        else:\n",
    "            delta_neg = delta_an\n",
    "\n",
    "        exp_ap = torch.exp(delta_ap)\n",
    "        exp_neg = torch.exp(delta_neg)\n",
    "        denom = exp_ap + exp_neg + self.eps\n",
    "\n",
    "        d_plus = exp_ap / denom  # expected → 0\n",
    "        d_minus = exp_neg / denom  # expected → 1\n",
    "\n",
    "        loss_vec = (d_plus**2) + ((d_minus - 1) ** 2)  # MSE vs (0,1)\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return loss_vec.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss_vec.sum()\n",
    "        else:  # 'none'\n",
    "            return loss_vec\n",
    "\n",
    "\n",
    "class Loc2VecTripletLoss(nn.Module):\n",
    "    def __init__(self, pos_target=0.0, neg_target=1.0):\n",
    "        super().__init__()\n",
    "        self.pos_target = pos_target\n",
    "        self.neg_target = neg_target\n",
    "\n",
    "    def forward(self, anchor_i, anchor_p, anchor_n):\n",
    "        distance_i_p = F.pairwise_distance(anchor_i, anchor_p)\n",
    "        distance_i_n = F.pairwise_distance(anchor_i, anchor_n)\n",
    "\n",
    "        loss = (\n",
    "            (distance_i_p - self.pos_target) ** 2\n",
    "            + (distance_i_n - self.neg_target) ** 2\n",
    "        ).mean()\n",
    "\n",
    "        np_distance_a_pos = distance_i_p.mean().item()\n",
    "        np_distance_a_neg = distance_i_n.mean().item()\n",
    "\n",
    "        loss_log = f\"LOSS: {loss.item():.3f} | (+) DIST: {np_distance_a_pos:.3f} | (-) DIST: {np_distance_a_neg:.3f}\"\n",
    "\n",
    "        return loss  # loss_log # np_distance_a_pos, np_distance_a_neg, distance_i_n.min().item()\n",
    "\n",
    "\n",
    "class EfficientNetLoc2Vec(nn.Module):\n",
    "    \"\"\"Transfer learning model using EfficientNet B0 as backbone for Loc2Vec embeddings.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels: int = 3,\n",
    "        embedding_dim: int = 16,\n",
    "        dropout_rate: float = 0.5,\n",
    "        pretrained: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        weights = EfficientNet_B0_Weights.DEFAULT if pretrained else None\n",
    "        self.backbone = models.efficientnet_b0(weights=weights)\n",
    "\n",
    "        if input_channels != 3:\n",
    "            original_conv = self.backbone.features[0][0]\n",
    "            self.backbone.features[0][0] = nn.Conv2d(\n",
    "                input_channels,\n",
    "                original_conv.out_channels,\n",
    "                kernel_size=original_conv.kernel_size,\n",
    "                stride=original_conv.stride,\n",
    "                padding=original_conv.padding,\n",
    "                bias=original_conv.bias is not None,\n",
    "            )\n",
    "\n",
    "        num_features = self.backbone.classifier[1].in_features\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_rate), nn.Linear(num_features, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.backbone(x)\n",
    "\n",
    "\n",
    "class EfficientNetV2SLoc2Vec(nn.Module):\n",
    "    \"\"\"Transfer learning model using EfficientNetV2-S as backbone for Loc2Vec embeddings.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels: int = 3,\n",
    "        embedding_dim: int = 16,\n",
    "        dropout_rate: float = 0.5,\n",
    "        pretrained: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        weights = EfficientNet_V2_S_Weights.DEFAULT if pretrained else None\n",
    "        self.backbone = models.efficientnet_v2_s(weights=weights)\n",
    "\n",
    "        if input_channels != 3:\n",
    "            original_conv = self.backbone.features[0][0]\n",
    "            self.backbone.features[0][0] = nn.Conv2d(\n",
    "                input_channels,\n",
    "                original_conv.out_channels,\n",
    "                kernel_size=original_conv.kernel_size,\n",
    "                stride=original_conv.stride,\n",
    "                padding=original_conv.padding,\n",
    "                bias=original_conv.bias is not None,\n",
    "            )\n",
    "\n",
    "        num_features = self.backbone.classifier[1].in_features\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_rate), nn.Linear(num_features, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.backbone(x)\n",
    "\n",
    "\n",
    "class EfficientNetV2MLoc2Vec(nn.Module):\n",
    "    \"\"\"Transfer learning model using EfficientNetV2-M as backbone for Loc2Vec embeddings.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels: int = 3,\n",
    "        embedding_dim: int = 16,\n",
    "        dropout_rate: float = 0.5,\n",
    "        pretrained: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        weights = EfficientNet_V2_M_Weights.DEFAULT if pretrained else None\n",
    "        self.backbone = models.efficientnet_v2_m(weights=weights)\n",
    "\n",
    "        if input_channels != 3:\n",
    "            original_conv = self.backbone.features[0][0]\n",
    "            self.backbone.features[0][0] = nn.Conv2d(\n",
    "                input_channels,\n",
    "                original_conv.out_channels,\n",
    "                kernel_size=original_conv.kernel_size,\n",
    "                stride=original_conv.stride,\n",
    "                padding=original_conv.padding,\n",
    "                bias=original_conv.bias is not None,\n",
    "            )\n",
    "\n",
    "        num_features = self.backbone.classifier[1].in_features\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_rate), nn.Linear(num_features, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.backbone(x)\n",
    "\n",
    "\n",
    "class ResNetLoc2Vec(nn.Module):\n",
    "    \"\"\"Transfer learning model using ResNet50 as backbone for Loc2Vec embeddings.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels: int = 3,\n",
    "        embedding_dim: int = 16,\n",
    "        dropout_rate: float = 0.5,\n",
    "        pretrained: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        weights = ResNet50_Weights.DEFAULT if pretrained else None\n",
    "        self.backbone = models.resnet50(weights=weights)\n",
    "\n",
    "        if input_channels != 3:\n",
    "            original_conv = self.backbone.conv1\n",
    "            self.backbone.conv1 = nn.Conv2d(\n",
    "                input_channels,\n",
    "                original_conv.out_channels,\n",
    "                kernel_size=original_conv.kernel_size,\n",
    "                stride=original_conv.stride,\n",
    "                padding=original_conv.padding,\n",
    "                bias=original_conv.bias is not None,\n",
    "            )\n",
    "\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_rate), nn.Linear(num_features, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.backbone(x)\n",
    "\n",
    "\n",
    "class ConvNeXtLoc2Vec(nn.Module):\n",
    "    \"\"\"Transfer learning model using ConvNeXt-Small as backbone for Loc2Vec embeddings.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels: int = 3,\n",
    "        embedding_dim: int = 16,\n",
    "        dropout_rate: float = 0.5,\n",
    "        pretrained: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        weights = ConvNeXt_Small_Weights.DEFAULT if pretrained else None\n",
    "        self.backbone = models.convnext_small(weights=weights)\n",
    "\n",
    "        if input_channels != 3:\n",
    "            original_conv = self.backbone.features[0][0]\n",
    "            self.backbone.features[0][0] = nn.Conv2d(\n",
    "                input_channels,\n",
    "                original_conv.out_channels,\n",
    "                kernel_size=original_conv.kernel_size,\n",
    "                stride=original_conv.stride,\n",
    "                padding=original_conv.padding,\n",
    "                bias=original_conv.bias is not None,\n",
    "            )\n",
    "\n",
    "        # Fix ConvNeXt shape issue - replace classifier completely\n",
    "        num_features = self.backbone.classifier[2].in_features  # 768\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),  # Global average pooling\n",
    "            nn.Flatten(),  # Flatten to (batch_size, 768)\n",
    "            nn.LayerNorm(num_features),  # Layer normalization\n",
    "            nn.Dropout(p=dropout_rate),  # Dropout\n",
    "            nn.Linear(num_features, embedding_dim),  # Final linear layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.backbone(x)\n",
    "\n",
    "\n",
    "class SwinTransformerLoc2Vec(nn.Module):\n",
    "    \"\"\"Transfer learning model using Swin Transformer Small as backbone for Loc2Vec embeddings.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels: int = 3,\n",
    "        embedding_dim: int = 16,\n",
    "        dropout_rate: float = 0.5,\n",
    "        pretrained: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if input_channels != 3:\n",
    "            raise NotImplementedError(\"Swin Transformer only supports 3 input channels\")\n",
    "\n",
    "        weights = Swin_S_Weights.DEFAULT if pretrained else None\n",
    "        self.backbone = models.swin_s(weights=weights)\n",
    "\n",
    "        num_features = self.backbone.head.in_features\n",
    "        self.backbone.head = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_rate), nn.Linear(num_features, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.backbone(x)\n",
    "\n",
    "\n",
    "class MobileNetV3Loc2Vec(nn.Module):\n",
    "    \"\"\"Transfer learning model using MobileNetV3-Large as backbone for Loc2Vec embeddings.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels: int = 3,\n",
    "        embedding_dim: int = 16,\n",
    "        dropout_rate: float = 0.5,\n",
    "        pretrained: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        weights = MobileNet_V3_Large_Weights.DEFAULT if pretrained else None\n",
    "        self.backbone = models.mobilenet_v3_large(weights=weights)\n",
    "\n",
    "        if input_channels != 3:\n",
    "            original_conv = self.backbone.features[0][0]\n",
    "            self.backbone.features[0][0] = nn.Conv2d(\n",
    "                input_channels,\n",
    "                original_conv.out_channels,\n",
    "                kernel_size=original_conv.kernel_size,\n",
    "                stride=original_conv.stride,\n",
    "                padding=original_conv.padding,\n",
    "                bias=original_conv.bias is not None,\n",
    "            )\n",
    "\n",
    "        num_features = self.backbone.classifier[3].in_features\n",
    "        self.backbone.classifier[3] = nn.Linear(num_features, embedding_dim)\n",
    "        self.backbone.classifier[0] = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.backbone(x)\n",
    "\n",
    "\n",
    "class MobileNetV3SmallLoc2Vec(nn.Module):\n",
    "    \"\"\"Transfer learning model using MobileNetV3-Small as backbone for Loc2Vec embeddings.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels: int = 3,\n",
    "        embedding_dim: int = 16,\n",
    "        dropout_rate: float = 0.5,\n",
    "        pretrained: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        weights = MobileNet_V3_Small_Weights.DEFAULT if pretrained else None\n",
    "        self.backbone = models.mobilenet_v3_small(weights=weights)\n",
    "\n",
    "        if input_channels != 3:\n",
    "            original_conv = self.backbone.features[0][0]\n",
    "            self.backbone.features[0][0] = nn.Conv2d(\n",
    "                input_channels,\n",
    "                original_conv.out_channels,\n",
    "                kernel_size=original_conv.kernel_size,\n",
    "                stride=original_conv.stride,\n",
    "                padding=original_conv.padding,\n",
    "                bias=original_conv.bias is not None,\n",
    "            )\n",
    "\n",
    "        # Fix MobileNetV3-Small shape issue\n",
    "        num_features = self.backbone.classifier[0].in_features  # 576\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.Linear(num_features, num_features // 2),  # 576 -> 288\n",
    "            nn.Hardswish(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(num_features // 2, embedding_dim),  # 288 -> 16\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, loss_fn, device, scheduler=None):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The Pytorch model instance\n",
    "        train_loader (DataLoader): DataLoader for training data\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for updating model parameters\n",
    "        loss_fn (nn.Module): Loss function to compute the loss\n",
    "        device (torch.device): Device to run the training on\n",
    "\n",
    "    Returns:\n",
    "        float: Average loss for the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=\"Training\", total=len(train_loader)):\n",
    "        # Move data to the specified device\n",
    "        anchor = batch[\"anchor_image\"].to(device)\n",
    "        positive = batch[\"pos_image\"].to(device)\n",
    "        negative = batch[\"neg_image\"].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        anchor_out = model(anchor)\n",
    "        positive_out = model(positive)\n",
    "        negative_out = model(negative)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(anchor_out, positive_out, negative_out)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(filename) -> ImageFile:\n",
    "    return Image.open(filename).convert(\"RGB\")\n",
    "\n",
    "\n",
    "def list_tiles_to_df(tiles_dir):\n",
    "    files = []\n",
    "    for file in Path(tiles_dir).resolve().glob(\"**/*.png\"):\n",
    "        # if filesize is equal to 103 bytes, skip it\n",
    "        if file.stat().st_size == 103:  # empty tile\n",
    "            continue\n",
    "\n",
    "        files.append((file.parent.name, file.stem, file.parent.parent.name, str(file)))\n",
    "\n",
    "    return pd.DataFrame(files, columns=[\"x\", \"y\", \"zoom\", \"filename\"])\n",
    "\n",
    "\n",
    "class OptimizedTilesDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tiles_root_dir,\n",
    "        pos_radius=1,\n",
    "        neg_radius_min=10,\n",
    "        transform=None,\n",
    "        preload_images=True,\n",
    "    ):\n",
    "        self.tiles_root_dir = tiles_root_dir\n",
    "        self.df = list_tiles_to_df(tiles_root_dir)\n",
    "        self.pos_radius = pos_radius\n",
    "        self.neg_radius_min = neg_radius_min\n",
    "        self.transform = transform\n",
    "        self.preload_images = preload_images\n",
    "\n",
    "        print(f\"Found {len(self.df)} valid tiles\")\n",
    "\n",
    "        # Convert coordinates to integers once\n",
    "        self.df[\"x_int\"] = self.df[\"x\"].astype(int)\n",
    "        self.df[\"y_int\"] = self.df[\"y\"].astype(int)\n",
    "\n",
    "        # Preload all images if requested\n",
    "        if preload_images:\n",
    "            print(\"Preloading all images into memory...\")\n",
    "            self.images = {}\n",
    "            self.positive_candidates = {}\n",
    "            self.negative_candidates = {}\n",
    "\n",
    "            self._preload_images()\n",
    "            self._precompute_candidates()\n",
    "            print(\"Preloading complete!\")\n",
    "        else:\n",
    "            self.images = None\n",
    "            self._precompute_candidates_lazy()\n",
    "\n",
    "    def _preload_images(self):\n",
    "        \"\"\"Load all images into memory during initialization.\"\"\"\n",
    "        from tqdm import tqdm\n",
    "\n",
    "        for idx, row in tqdm(\n",
    "            self.df.iterrows(), total=len(self.df), desc=\"Loading images\"\n",
    "        ):\n",
    "            filename = row[\"filename\"]\n",
    "            try:\n",
    "                self.images[idx] = load_image(filename)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to load {filename}: {e}\")\n",
    "                # Create a blank image as fallback\n",
    "                self.images[idx] = Image.new(\"RGB\", (256, 256), color=\"black\")\n",
    "\n",
    "    def _precompute_candidates(self):\n",
    "        \"\"\"Precompute positive and negative candidates for each sample.\"\"\"\n",
    "        from tqdm import tqdm\n",
    "\n",
    "        for idx, row in tqdm(\n",
    "            self.df.iterrows(), total=len(self.df), desc=\"Computing candidates\"\n",
    "        ):\n",
    "            x, y, zoom = row[\"x_int\"], row[\"y_int\"], row[\"zoom\"]\n",
    "\n",
    "            # Find positive candidates (within pos_radius)\n",
    "            pos_mask = (\n",
    "                (self.df[\"zoom\"] == zoom)\n",
    "                & (abs(self.df[\"x_int\"] - x) <= self.pos_radius)\n",
    "                & (abs(self.df[\"y_int\"] - y) <= self.pos_radius)\n",
    "                & (self.df.index != idx)  # Don't include self\n",
    "            )\n",
    "            pos_indices = self.df[pos_mask].index.tolist()\n",
    "\n",
    "            # If no positive candidates, use self as fallback\n",
    "            if not pos_indices:\n",
    "                pos_indices = [idx]\n",
    "\n",
    "            self.positive_candidates[idx] = pos_indices\n",
    "\n",
    "            # Find negative candidates (outside neg_radius_min)\n",
    "            neg_mask = (\n",
    "                (self.df[\"zoom\"] == zoom)\n",
    "                & (\n",
    "                    (abs(self.df[\"x_int\"] - x) > self.neg_radius_min)\n",
    "                    | (abs(self.df[\"y_int\"] - y) > self.neg_radius_min)\n",
    "                )\n",
    "                & (self.df.index != idx)  # Don't include self\n",
    "            )\n",
    "            neg_indices = self.df[neg_mask].index.tolist()\n",
    "\n",
    "            # If no suitable negatives, use all others as candidates\n",
    "            if not neg_indices:\n",
    "                neg_indices = [i for i in self.df.index if i != idx]\n",
    "\n",
    "            self.negative_candidates[idx] = neg_indices\n",
    "\n",
    "    def _precompute_candidates_lazy(self):\n",
    "        \"\"\"Lighter version that precomputes candidate indices without loading images.\"\"\"\n",
    "        from tqdm import tqdm\n",
    "\n",
    "        self.positive_candidates = {}\n",
    "        self.negative_candidates = {}\n",
    "\n",
    "        for idx, row in tqdm(\n",
    "            self.df.iterrows(), total=len(self.df), desc=\"Computing candidates\"\n",
    "        ):\n",
    "            x, y, zoom = row[\"x_int\"], row[\"y_int\"], row[\"zoom\"]\n",
    "\n",
    "            # Find positive candidates\n",
    "            pos_mask = (\n",
    "                (self.df[\"zoom\"] == zoom)\n",
    "                & (abs(self.df[\"x_int\"] - x) <= self.pos_radius)\n",
    "                & (abs(self.df[\"y_int\"] - y) <= self.pos_radius)\n",
    "                & (self.df.index != idx)\n",
    "            )\n",
    "            pos_indices = self.df[pos_mask].index.tolist()\n",
    "            if not pos_indices:\n",
    "                pos_indices = [idx]\n",
    "            self.positive_candidates[idx] = pos_indices\n",
    "\n",
    "            # Find negative candidates\n",
    "            neg_mask = (\n",
    "                (self.df[\"zoom\"] == zoom)\n",
    "                & (\n",
    "                    (abs(self.df[\"x_int\"] - x) > self.neg_radius_min)\n",
    "                    | (abs(self.df[\"y_int\"] - y) > self.neg_radius_min)\n",
    "                )\n",
    "                & (self.df.index != idx)\n",
    "            )\n",
    "            neg_indices = self.df[neg_mask].index.tolist()\n",
    "            if not neg_indices:\n",
    "                neg_indices = [i for i in self.df.index if i != idx]\n",
    "            self.negative_candidates[idx] = neg_indices\n",
    "\n",
    "    def _get_image(self, idx):\n",
    "        \"\"\"Get image either from preloaded cache or load on demand.\"\"\"\n",
    "        if self.preload_images:\n",
    "            return self.images[idx]\n",
    "        else:\n",
    "            filename = self.df.iloc[idx][\"filename\"]\n",
    "            try:\n",
    "                return load_image(filename)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to load {filename}: {e}\")\n",
    "                return Image.new(\"RGB\", (256, 256), color=\"black\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get anchor image\n",
    "        anchor_image = self._get_image(idx)\n",
    "\n",
    "        # Get positive sample (random choice from precomputed candidates)\n",
    "        pos_idx = random.choice(self.positive_candidates[idx])\n",
    "        pos_image = self._get_image(pos_idx)\n",
    "\n",
    "        # Get negative sample (random choice from precomputed candidates)\n",
    "        neg_idx = random.choice(self.negative_candidates[idx])\n",
    "        neg_image = self._get_image(neg_idx)\n",
    "\n",
    "        # Apply transforms if specified\n",
    "        if self.transform:\n",
    "            anchor_image = self.transform(anchor_image)\n",
    "            pos_image = self.transform(pos_image)\n",
    "            neg_image = self.transform(neg_image)\n",
    "\n",
    "        # Get metadata\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        return {\n",
    "            \"anchor_image\": anchor_image,\n",
    "            \"pos_image\": pos_image,\n",
    "            \"neg_image\": neg_image,\n",
    "            \"x\": row[\"x\"],\n",
    "            \"y\": row[\"y\"],\n",
    "            \"zoom\": row[\"zoom\"],\n",
    "            \"filename\": row[\"filename\"],\n",
    "        }\n",
    "\n",
    "\n",
    "# Backward compatibility - use optimized version by default\n",
    "class TilesDataset(OptimizedTilesDataset):\n",
    "    def __init__(self, tiles_root_dir, pos_radius=1, neg_radius_min=10, transform=None):\n",
    "        # Default to preloading images for maximum performance\n",
    "        super().__init__(\n",
    "            tiles_root_dir, pos_radius, neg_radius_min, transform, preload_images=True\n",
    "        )\n",
    "\n",
    "\n",
    "# Memory-efficient version for very large datasets\n",
    "class LazyTilesDataset(OptimizedTilesDataset):\n",
    "    def __init__(self, tiles_root_dir, pos_radius=1, neg_radius_min=10, transform=None):\n",
    "        # Don't preload images, but still precompute candidates\n",
    "        super().__init__(\n",
    "            tiles_root_dir, pos_radius, neg_radius_min, transform, preload_images=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Training and Benchmarking Loc2Vec Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_model(model_class, model_name, train_loader, device, epochs=3):\n",
    "    \"\"\"Benchmark a single model configuration\"\"\"\n",
    "    results = []\n",
    "\n",
    "    base_lr = 1e-4  # Standard LR for pre-trained models\n",
    "\n",
    "    # Test different optimizers and schedulers\n",
    "    configs = [\n",
    "        {\"optimizer\": \"Adam\", \"lr\": base_lr, \"scheduler\": None},\n",
    "        {\"optimizer\": \"AdamW\", \"lr\": base_lr, \"scheduler\": None},\n",
    "    ]\n",
    "\n",
    "    for config in configs:\n",
    "        try:\n",
    "            # Clear memory\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "            # Initialize model\n",
    "            model = model_class(input_channels=3, embedding_dim=16, dropout_rate=0.5)\n",
    "            model.to(device)\n",
    "\n",
    "            # Count parameters\n",
    "            param_count = count_parameters(model)\n",
    "\n",
    "            # Setup optimizer\n",
    "            if config[\"optimizer\"] == \"AdamW\":\n",
    "                optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "            else:  # AdamW\n",
    "                optimizer = optim.AdamW(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "            # Setup loss function (Step 3 - More stable loss)\n",
    "            loss_fn = SoftmaxTripletLoss()\n",
    "\n",
    "            # Memory before training\n",
    "            memory_before = get_memory_usage()\n",
    "\n",
    "            # Training with gradient clipping (Step 2)\n",
    "            start_time = time.time()\n",
    "            epoch_losses = []\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                epoch_loss = train_epoch(\n",
    "                    model, train_loader, optimizer, loss_fn, device, scheduler=None\n",
    "                )\n",
    "                # Add gradient clipping after training step\n",
    "                epoch_losses.append(epoch_loss)\n",
    "\n",
    "            training_time = time.time() - start_time\n",
    "\n",
    "            # Better loss analysis\n",
    "            avg_loss = np.mean(epoch_losses)\n",
    "            final_loss = epoch_losses[-1] if epoch_losses else 0\n",
    "            loss_std = np.std(epoch_losses) if len(epoch_losses) > 1 else 0\n",
    "            min_loss = np.min(epoch_losses) if epoch_losses else 0\n",
    "            max_loss = np.max(epoch_losses) if epoch_losses else 0\n",
    "            loss_trend = (\n",
    "                \"Improving\"\n",
    "                if len(epoch_losses) > 1 and epoch_losses[-1] < epoch_losses[0]\n",
    "                else \"Stable/Degrading\"\n",
    "            )\n",
    "            loss_string = \", \".join([f\"{loss:.4f}\" for loss in epoch_losses])\n",
    "\n",
    "            # Memory after training\n",
    "            memory_after = get_memory_usage()\n",
    "            memory_used = memory_after - memory_before\n",
    "\n",
    "            # Evaluate embedding quality\n",
    "            silhouette_avg = evaluate_embeddings(model, train_loader, device)\n",
    "\n",
    "            # Record results\n",
    "            result = {\n",
    "                \"Model\": model_name,\n",
    "                \"Optimizer\": config[\"optimizer\"],\n",
    "                \"Scheduler\": config[\"scheduler\"] or \"None\",\n",
    "                \"Parameters (M)\": param_count / 1e6,\n",
    "                \"Training Time (s)\": training_time,\n",
    "                \"Memory Used (MB)\": memory_used,\n",
    "                \"Final Loss\": final_loss,\n",
    "                \"Avg Loss\": avg_loss,\n",
    "                \"Min Loss\": min_loss,\n",
    "                \"Max Loss\": max_loss,\n",
    "                \"Loss Std\": loss_std,\n",
    "                \"Loss Trend\": loss_trend,\n",
    "                \"All Losses\": loss_string,\n",
    "                \"Silhouette Score\": silhouette_avg,\n",
    "                \"Time per Epoch (s)\": training_time / epochs,\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "            print(\n",
    "                f\"✓ {model_name} - {config['optimizer']} - {config['scheduler'] or 'None'}\"\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"✗ {model_name} - {config['optimizer']} - {config['scheduler'] or 'None'}: {str(e)}\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comprehensive_comparison(train_loader, device):\n",
    "    \"\"\"Run comprehensive comparison of all models\"\"\"\n",
    "\n",
    "    # Model configurations\n",
    "    models_to_test = [\n",
    "        (Loc2VecModel, \"Custom Loc2Vec\"),\n",
    "        # (EfficientNetLoc2Vec, \"EfficientNet B0\"),\n",
    "        # (EfficientNetV2SLoc2Vec, \"EfficientNetV2-S\"),\n",
    "        # (EfficientNetV2MLoc2Vec, \"EfficientNetV2-M\"),\n",
    "        # (ResNetLoc2Vec, \"ResNet50\"),\n",
    "        # (ConvNeXtLoc2Vec, \"ConvNeXt-Small\"),\n",
    "        # (SwinTransformerLoc2Vec, \"Swin-Small\"),\n",
    "        # (MobileNetV3Loc2Vec, \"MobileNetV3-Large\"),\n",
    "        # (MobileNetV3SmallLoc2Vec, \"MobileNetV3-Small\"),\n",
    "    ]\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    print(\"Starting comprehensive model comparison...\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for model_class, model_name in models_to_test:\n",
    "        print(f\"\\nTesting {model_name}...\")\n",
    "        try:\n",
    "            results = benchmark_model(model_class, model_name, train_loader, device)\n",
    "        except Exception as e:\n",
    "            print(f\"Error benchmarking {model_name}: {e}\")\n",
    "            continue\n",
    "        else:\n",
    "            all_results.extend(results)\n",
    "\n",
    "    # Create DataFrame and sort by performance\n",
    "    df = pd.DataFrame(all_results)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"COMPREHENSIVE COMPARISON RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\n📊 FULL RESULTS TABLE:\")\n",
    "    print(df.round(4).to_string(index=False))\n",
    "\n",
    "    # Best performers analysis\n",
    "    print(\"\\n🏆 BEST PERFORMERS BY CATEGORY:\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    best_speed = df.loc[df[\"Training Time (s)\"].idxmin()]\n",
    "    print(\n",
    "        f\"⚡ Fastest: {best_speed['Model']} ({best_speed['Optimizer']}) - {best_speed['Training Time (s)']:.2f}s\"\n",
    "    )\n",
    "\n",
    "    best_memory = df.loc[df[\"Memory Used (MB)\"].idxmin()]\n",
    "    print(\n",
    "        f\"💾 Most Memory Efficient: {best_memory['Model']} ({best_memory['Optimizer']}) - {best_memory['Memory Used (MB)']:.1f}MB\"\n",
    "    )\n",
    "\n",
    "    best_final_loss = df.loc[df[\"Final Loss\"].idxmin()]\n",
    "    print(\n",
    "        f\"🎯 Best Final Loss: {best_final_loss['Model']} ({best_final_loss['Optimizer']}) - {best_final_loss['Final Loss']:.4f}\"\n",
    "    )\n",
    "\n",
    "    most_stable = df.loc[df[\"Loss Std\"].idxmin()]\n",
    "    print(\n",
    "        f\"📈 Most Stable Training: {most_stable['Model']} ({most_stable['Optimizer']}) - Std: {most_stable['Loss Std']:.4f}\"\n",
    "    )\n",
    "\n",
    "    best_silhouette = df.loc[df[\"Silhouette Score\"].idxmax()]\n",
    "    print(\n",
    "        f\"🎨 Best Embeddings: {best_silhouette['Model']} ({best_silhouette['Optimizer']}) - {best_silhouette['Silhouette Score']:.4f}\"\n",
    "    )\n",
    "\n",
    "    smallest_model = df.loc[df[\"Parameters (M)\"].idxmin()]\n",
    "    print(\n",
    "        f\"📦 Smallest Model: {smallest_model['Model']} - {smallest_model['Parameters (M)']:.1f}M params\"\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "dataset = TilesDataset(\n",
    "    \"full\",\n",
    "    pos_radius=1,\n",
    "    transform=T.Compose(\n",
    "        [\n",
    "            T.Resize((128, 128)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.8107, 0.8611, 0.7814], [0.1215, 0.0828, 0.1320]),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "import multiprocessing as mp\n",
    "\n",
    "try:\n",
    "    mp.set_start_method(\"fork\", force=True)\n",
    "except RuntimeError:\n",
    "    pass  # Already set\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    prefetch_factor=10,\n",
    "    persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_comprehensive_comparison(train_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
