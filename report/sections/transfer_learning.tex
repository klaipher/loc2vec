\subsection{Transfer Learning Experimental Design}

Our transfer learning experiments aimed to systematically evaluate how pre-trained CNN architectures perform when adapted for location embedding tasks. We selected three representative architectures that span different design philosophies and computational requirements.

\subsubsection{Architecture Selection Rationale}

\textbf{EfficientNet-B0} \cite{tan2019efficientnet}:
\begin{itemize}
    \item Represents compound scaling approach
    \item Optimized for efficiency-accuracy trade-offs
    \item 5.3M parameters, 390 MFLOPs
    \item Known for strong transfer learning performance
\end{itemize}

\textbf{ResNet-50} \cite{he2016resnet}:
\begin{itemize}
    \item Classic residual learning architecture
    \item Well-established baseline for transfer learning
    \item 25.6M parameters, 4.1 GFLOPs
    \item Strong feature extraction capabilities
\end{itemize}

\textbf{MobileNetv3-Large} \cite{howard2019mobilenetv3}:
\begin{itemize}
    \item Mobile-optimized architecture
    \item Depthwise separable convolutions
    \item 5.4M parameters, 217 MFLOPs
    \item Optimized for inference speed and memory efficiency
\end{itemize}

\subsubsection{Input Adaptation Strategy}

For each architecture, we implemented a systematic input adaptation approach to handle 12-channel geographical data instead of 3-channel RGB images:

\textbf{Method 1: Channel Expansion}
\begin{lstlisting}[language=Python, caption=Channel Expansion Strategy]
def adapt_first_layer_expansion(model, in_channels=12):
    """Expand first conv layer to accept 12 channels"""
    first_conv = model.features[0][0]  # EfficientNet example

    # Create new conv layer with expanded input
    new_conv = nn.Conv2d(
        in_channels=in_channels,
        out_channels=first_conv.out_channels,
        kernel_size=first_conv.kernel_size,
        stride=first_conv.stride,
        padding=first_conv.padding,
        bias=first_conv.bias is not None
    )

    # Initialize new channels with average of RGB weights
    with torch.no_grad():
        rgb_weights = first_conv.weight.data
        avg_weight = rgb_weights.mean(dim=1, keepdim=True)

        # Replicate average weight for each new channel
        new_weights = avg_weight.repeat(1, in_channels, 1, 1)
        new_conv.weight.data = new_weights

    return new_conv
\end{lstlisting}

\textbf{Method 2: Learned Projection}
\begin{lstlisting}[language=Python, caption=Learned Projection Strategy]
class ChannelProjection(nn.Module):
    def __init__(self, in_channels=12, out_channels=3):
        super().__init__()
        self.projection = nn.Conv2d(in_channels, out_channels, 1)

    def forward(self, x):
        return self.projection(x)

# Usage: Add projection layer before pretrained model
model = nn.Sequential(
    ChannelProjection(12, 3),
    pretrained_backbone
)
\end{lstlisting}

We evaluated both approaches and found that channel expansion provided better performance, likely due to the preservation of all geographical information.

\subsection{Transfer Learning Results}

\subsubsection{Overall Performance Comparison}

\begin{table}[H]
\centering
\caption{Transfer Learning Architecture Comparison}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{Architecture} & \textbf{Triplet Acc} & \textbf{Venue Acc@1} & \textbf{Venue Acc@5} & \textbf{MRR} & \textbf{Training Time (h)} \\
\hline
Base CNN & 0.892 & 0.634 & 0.821 & 0.715 & 24.3 \\
EfficientNet-B0 & \textbf{0.931} & \textbf{0.687} & \textbf{0.856} & \textbf{0.762} & 18.7 \\
ResNet-50 & 0.915 & 0.671 & 0.842 & 0.748 & 28.9 \\
MobileNetv3-Large & 0.908 & 0.658 & 0.834 & 0.738 & 16.2 \\
\hline
\end{tabular}
\end{table}

\textbf{Key Findings}:
\begin{itemize}
    \item All transfer learning approaches outperformed the base CNN
    \item EfficientNet-B0 achieved the best overall performance across all metrics
    \item MobileNetv3 provided the best training efficiency while maintaining competitive accuracy
    \item ResNet-50 showed strong performance but with higher computational cost
\end{itemize}

\subsubsection{Convergence Analysis}

\begin{figure}[H]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Architecture} & \textbf{Epochs to 90\% Final Acc} & \textbf{Final Validation Loss} & \textbf{Overfitting Score} \\
\hline
Base CNN & 65 & 0.095 & 0.12 \\
EfficientNet-B0 & 42 & 0.068 & 0.08 \\
ResNet-50 & 38 & 0.072 & 0.09 \\
MobileNetv3-Large & 45 & 0.074 & 0.10 \\
\hline
\end{tabular}
\caption{Convergence Characteristics}
\end{figure}

Transfer learning models demonstrated faster convergence and better regularization:
\begin{itemize}
    \item 35-40\% faster convergence compared to training from scratch
    \item Lower final validation loss indicating better generalization
    \item Reduced overfitting due to pre-trained feature extractors
\end{itemize}

\subsubsection{Detailed EfficientNet Analysis}

Given EfficientNet's superior performance, we conducted detailed analysis:

\textbf{Layer-wise Feature Evolution}:
\begin{table}[H]
\centering
\caption{EfficientNet Feature Analysis by Layer}
\begin{tabular}{|l|c|l|}
\hline
\textbf{Layer Group} & \textbf{Feature Map Size} & \textbf{Learned Features} \\
\hline
Block 1-2 & 128×128 & Edge detection, basic shapes \\
Block 3-4 & 64×64 & Road patterns, building clusters \\
Block 5-6 & 32×32 & Land use boundaries, neighborhoods \\
Block 7-8 & 16×16 & District-level patterns \\
Global Pool & 1×1 & High-level semantic features \\
\hline
\end{tabular}
\end{table}

\textbf{Channel Importance Analysis}:
We analyzed which geographical channels contributed most to EfficientNet's performance:

\begin{table}[H]
\centering
\caption{Channel Importance for EfficientNet}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Channel} & \textbf{Importance Score} & \textbf{Performance Drop w/o Channel} \\
\hline
Roads Primary & 0.234 & -8.3\% \\
Buildings & 0.198 & -6.7\% \\
Commercial Areas & 0.167 & -5.2\% \\
Roads Secondary & 0.143 & -4.1\% \\
Residential Areas & 0.132 & -3.8\% \\
Water Bodies & 0.089 & -2.1\% \\
Green Spaces & 0.037 & -1.2\% \\
\hline
\end{tabular}
\end{table}

\subsection{Computational Efficiency Analysis}

\subsubsection{Training Efficiency}

\begin{table}[H]
\centering
\caption{Training Computational Requirements}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{Architecture} & \textbf{GPU Memory (GB)} & \textbf{FLOPs (G)} & \textbf{Params (M)} & \textbf{Epoch Time (min)} & \textbf{Total Energy (kWh)} \\
\hline
Base CNN & 6.8 & 2.1 & 2.1 & 14.6 & 5.8 \\
EfficientNet-B0 & 8.2 & 0.4 & 5.3 & 11.2 & 4.2 \\
ResNet-50 & 12.4 & 4.1 & 25.6 & 17.3 & 9.6 \\
MobileNetv3-Large & 7.1 & 0.2 & 5.4 & 9.7 & 3.1 \\
\hline
\end{tabular}
\end{table}

\subsubsection{Inference Performance}

\begin{table}[H]
\centering
\caption{Inference Performance Comparison}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Architecture} & \textbf{GPU Latency (ms)} & \textbf{CPU Latency (ms)} & \textbf{Mobile Latency (ms)} & \textbf{Model Size (MB)} \\
\hline
Base CNN & 3.2 & 28.5 & 145.0 & 8.4 \\
EfficientNet-B0 & 4.1 & 35.2 & 168.5 & 21.2 \\
ResNet-50 & 8.7 & 89.4 & 524.8 & 102.5 \\
MobileNetv3-Large & 2.8 & 24.1 & 112.3 & 21.6 \\
\hline
\end{tabular}
\end{table}

\textbf{Mobile Deployment Analysis}:
For mobile deployment scenarios, we evaluated models on simulated mobile hardware:
\begin{itemize}
    \item \textbf{MobileNetv3}: Best mobile performance with 112ms latency
    \item \textbf{Base CNN}: Good balance of accuracy and mobile efficiency
    \item \textbf{EfficientNet}: Acceptable for high-end mobile devices
    \item \textbf{ResNet-50}: Too slow for real-time mobile applications
\end{itemize}

\subsection{Fine-tuning Strategy Analysis}

\subsubsection{Progressive Unfreezing vs. Full Fine-tuning}

We compared different fine-tuning strategies:

\begin{table}[H]
\centering
\caption{Fine-tuning Strategy Comparison (EfficientNet-B0)}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Strategy} & \textbf{Final Accuracy} & \textbf{Convergence Time} & \textbf{Stability} & \textbf{Overfitting Risk} \\
\hline
Freeze backbone & 0.854 & Fast & High & Low \\
Progressive unfreeze & 0.931 & Medium & High & Medium \\
Full fine-tuning & 0.923 & Slow & Medium & High \\
Layer-wise LR & 0.928 & Medium & High & Medium \\
\hline
\end{tabular}
\end{table}

\textbf{Progressive Unfreezing Schedule}:
\begin{enumerate}
    \item Epochs 1-10: Freeze backbone, train only classifier head
    \item Epochs 11-30: Unfreeze top 3 blocks, reduce LR by 10×
    \item Epochs 31-60: Unfreeze all layers, reduce LR by 5×
    \item Epochs 61-100: Fine-tune with very low LR (1e-5)
\end{enumerate}

\subsubsection{Learning Rate Sensitivity}

\begin{table}[H]
\centering
\caption{Learning Rate Impact on Transfer Learning}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Initial LR} & \textbf{EfficientNet Acc} & \textbf{ResNet Acc} & \textbf{MobileNet Acc} \\
\hline
1e-2 & 0.876 & 0.891 & 0.863 \\
1e-3 & 0.931 & 0.915 & 0.908 \\
1e-4 & 0.918 & 0.907 & 0.895 \\
1e-5 & 0.887 & 0.882 & 0.879 \\
\hline
\end{tabular}
\end{table}

Optimal learning rate for transfer learning was consistently 1e-3, which is 10× lower than typical training from scratch.

\subsection{Geographic Generalization Analysis}

\subsubsection{Cross-District Performance}

We evaluated how well models trained on one district generalize to others:

\begin{table}[H]
\centering
\caption{Cross-District Generalization (Train on Shevchenkivskyi, Test on Others)}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Test District} & \textbf{Base CNN} & \textbf{EfficientNet} & \textbf{ResNet} & \textbf{MobileNet} \\
\hline
Pecherskyi & 0.587 & 0.642 & 0.628 & 0.615 \\
Podilskyi & 0.564 & 0.621 & 0.607 & 0.598 \\
Obolonskyi & 0.523 & 0.578 & 0.567 & 0.554 \\
Darnytskyi & 0.489 & 0.542 & 0.531 & 0.521 \\
\hline
\end{tabular}
\end{table}

Transfer learning models showed better generalization across districts, with EfficientNet maintaining the strongest performance even in unseen areas.

\subsubsection{Semantic Consistency Analysis}

We analyzed embedding consistency for similar locations across different districts:

\textbf{Example: Residential Areas}
\begin{itemize}
    \item \textbf{Intra-district similarity}: 0.78 ± 0.12 (EfficientNet)
    \item \textbf{Inter-district similarity}: 0.71 ± 0.15 (EfficientNet)
    \item \textbf{Baseline comparison}: 0.65 ± 0.18 (Base CNN)
\end{itemize}

\subsection{Error Analysis for Transfer Learning}

\subsubsection{Failure Case Analysis}

Transfer learning models showed improved robustness but still exhibited some failure patterns:

\textbf{Reduced Failures}:
\begin{itemize}
    \item Better handling of boundary areas (8.2\% error vs. 12.3\% for base model)
    \item Improved performance on sparse features (9.1\% vs. 13.7\%)
    \item More robust to noisy geographical data
\end{itemize}

\textbf{Persistent Challenges}:
\begin{itemize}
    \item New construction areas still challenging (11.8\% error rate)
    \item Fine-grained commercial distinction remains difficult
    \item Performance degrades in rural/suburban transitions
\end{itemize}

\subsubsection{Embedding Quality Comparison}

\begin{table}[H]
\centering
\caption{Embedding Quality Metrics Comparison}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Metric} & \textbf{Base CNN} & \textbf{EfficientNet} & \textbf{ResNet} & \textbf{MobileNet} \\
\hline
Silhouette Score & 0.634 & 0.712 & 0.695 & 0.678 \\
Calinski-Harabasz & 1,234 & 1,567 & 1,489 & 1,423 \\
Davies-Bouldin & 0.423 & 0.367 & 0.381 & 0.394 \\
Embedding Norm Std & 0.089 & 0.056 & 0.063 & 0.071 \\
\hline
\end{tabular}
\end{table}

The results demonstrate that transfer learning, particularly with EfficientNet, provides substantial improvements in both accuracy and embedding quality while maintaining reasonable computational requirements for deployment scenarios.
