\subsection{Development Environment and Tools}

Our implementation was developed using the following technology stack:

\textbf{Programming Languages and Frameworks}:
\begin{itemize}
    \item Python 3.8+ for core implementation
    \item PyTorch 1.12+ for deep learning models
    \item NumPy and SciPy for numerical computations
    \item Pandas for data manipulation and analysis
\end{itemize}

\textbf{Geospatial Data Processing}:
\begin{itemize}
    \item PostGIS 3.0+ with PostgreSQL 13+ for spatial database
    \item Mapnik 3.1+ for rasterization and map rendering
    \item GDAL/OGR for geospatial data format conversion
    \item Shapely and Fiona for geometric operations
    \item OSMnx for OpenStreetMap data extraction
\end{itemize}

\textbf{Machine Learning and Visualization}:
\begin{itemize}
    \item scikit-learn for evaluation metrics and preprocessing
    \item matplotlib and seaborn for visualization
    \item tensorboard for training monitoring
    \item torchvision for pre-trained models and transforms
\end{itemize}

\subsection{Data Acquisition and Preprocessing}

\subsubsection{OpenStreetMap Data Setup}

We established a comprehensive pipeline for acquiring and processing geographical data for Kyiv:

\begin{lstlisting}[language=Python, caption=OSM Data Download and Setup]
import osmium
import psycopg2
from sqlalchemy import create_engine

def setup_kyiv_database():
    # Download Ukraine OSM data
    ukraine_pbf = download_osm_data("ukraine-latest.osm.pbf")

    # Extract Kyiv region (bounding box)
    kyiv_bounds = {
        'north': 50.6195,
        'south': 50.2134,
        'east': 30.8251,
        'west': 30.0961
    }

    # Import to PostGIS database
    engine = create_engine(f"postgresql://{user}:{password}@localhost/{db}")

    # Create spatial tables
    setup_spatial_tables(engine, kyiv_bounds)

    return engine
\end{lstlisting}

\subsubsection{Feature Categorization and Channel Definition}

Based on analysis of Kyiv's urban characteristics and OSM data availability, we defined 12 semantic channels:

\begin{table}[H]
\centering
\caption{Geographic Feature Channels}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Channel} & \textbf{OSM Tags} & \textbf{Description} \\
\hline
Roads Primary & highway=primary,trunk & Major roads and highways \\
Roads Secondary & highway=secondary,tertiary & Secondary road network \\
Roads Local & highway=residential,service & Local and residential streets \\
Buildings & building=* & All building footprints \\
Water Bodies & natural=water,waterway=* & Rivers, lakes, streams \\
Green Spaces & leisure=park,natural=forest & Parks and natural areas \\
Commercial & landuse=commercial,retail & Commercial and retail areas \\
Residential & landuse=residential & Residential districts \\
Industrial & landuse=industrial & Industrial zones \\
Transportation & railway=*,aeroway=* & Rails, airports, stations \\
Amenities & amenity=*,tourism=* & POIs and public facilities \\
Historical & historic=*,tourism=heritage & Historical and cultural sites \\
\hline
\end{tabular}
\end{table}

\subsubsection{Rasterization Implementation}

We implemented a flexible rasterization system using Mapnik with custom stylesheets:

\begin{lstlisting}[language=Python, caption=Multi-channel Rasterization]
import mapnik
import numpy as np

class KyivRasterizer:
    def __init__(self, db_connection, tile_size=256):
        self.db_connection = db_connection
        self.tile_size = tile_size
        self.channel_configs = self._load_channel_configs()

    def generate_tile(self, lat, lon, radius_m=500):
        """Generate 12-channel raster tile for location"""
        # Calculate bounding box
        bbox = self._get_bbox(lat, lon, radius_m)

        # Initialize output tensor
        tile = np.zeros((self.tile_size, self.tile_size, 12))

        # Render each channel
        for i, channel_config in enumerate(self.channel_configs):
            channel_map = mapnik.Map(self.tile_size, self.tile_size)
            self._apply_style(channel_map, channel_config)
            channel_map.zoom_to_box(bbox)

            # Render to numpy array
            surface = mapnik.Image(self.tile_size, self.tile_size)
            mapnik.render(channel_map, surface)

            # Convert to binary mask
            img_array = np.frombuffer(surface.tostring(), dtype=np.uint8)
            img_array = img_array.reshape(self.tile_size, self.tile_size, 4)
            tile[:, :, i] = (img_array[:, :, 0] > 0).astype(np.float32)

        return tile

    def _apply_style(self, map_obj, channel_config):
        """Apply styling for specific channel"""
        # Implementation details for Mapnik styling
        pass
\end{lstlisting}

\subsection{Model Architecture Implementation}

\subsubsection{Base CNN Encoder}

Our base encoder implementation follows the methodology section design:

\begin{lstlisting}[language=Python, caption=Base CNN Encoder]
import torch
import torch.nn as nn
import torch.nn.functional as F

class Loc2VecEncoder(nn.Module):
    def __init__(self, embedding_dim=256):
        super(Loc2VecEncoder, self).__init__()

        # Convolutional layers
        self.conv1 = nn.Conv2d(12, 32, 3, padding=1)
        self.conv2 = nn.Conv2d(32, 32, 3, padding=1)

        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)
        self.conv4 = nn.Conv2d(64, 64, 3, padding=1)

        self.conv5 = nn.Conv2d(64, 128, 3, padding=1)
        self.conv6 = nn.Conv2d(128, 128, 3, padding=1)

        # Global pooling and fully connected layers
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        self.fc1 = nn.Linear(128, 512)
        self.fc2 = nn.Linear(512, embedding_dim)

        # Dropout for regularization
        self.dropout = nn.Dropout(0.3)

    def forward(self, x):
        # First block
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.max_pool2d(x, 2)

        # Second block
        x = F.relu(self.conv3(x))
        x = F.relu(self.conv4(x))
        x = F.max_pool2d(x, 2)

        # Third block
        x = F.relu(self.conv5(x))
        x = F.relu(self.conv6(x))
        x = F.max_pool2d(x, 2)

        # Global pooling and dense layers
        x = self.global_pool(x)
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)

        # L2 normalization
        x = F.normalize(x, p=2, dim=1)

        return x
\end{lstlisting}

\subsubsection{Transfer Learning Models}

We implemented adaptations for three pre-trained architectures:

\begin{lstlisting}[language=Python, caption=Transfer Learning Model Wrapper]
import torchvision.models as models

class TransferLearningEncoder(nn.Module):
    def __init__(self, architecture='efficientnet', embedding_dim=256):
        super(TransferLearningEncoder, self).__init__()

        if architecture == 'efficientnet':
            self.backbone = models.efficientnet_b0(pretrained=True)
            # Modify first conv layer for 12 channels
            self.backbone.features[0][0] = nn.Conv2d(
                12, 32, kernel_size=3, stride=2, padding=1, bias=False
            )
            # Replace classifier
            self.backbone.classifier = nn.Sequential(
                nn.Dropout(0.3),
                nn.Linear(1280, 512),
                nn.ReLU(),
                nn.Linear(512, embedding_dim)
            )

        elif architecture == 'resnet':
            self.backbone = models.resnet50(pretrained=True)
            # Modify first conv layer
            self.backbone.conv1 = nn.Conv2d(
                12, 64, kernel_size=7, stride=2, padding=3, bias=False
            )
            # Replace fc layer
            self.backbone.fc = nn.Sequential(
                nn.Dropout(0.3),
                nn.Linear(2048, 512),
                nn.ReLU(),
                nn.Linear(512, embedding_dim)
            )

        elif architecture == 'mobilenet':
            self.backbone = models.mobilenet_v3_large(pretrained=True)
            # Modify first conv layer
            self.backbone.features[0][0] = nn.Conv2d(
                12, 16, kernel_size=3, stride=2, padding=1, bias=False
            )
            # Replace classifier
            self.backbone.classifier = nn.Sequential(
                nn.Dropout(0.3),
                nn.Linear(960, 512),
                nn.ReLU(),
                nn.Linear(512, embedding_dim)
            )

    def forward(self, x):
        x = self.backbone(x)
        return F.normalize(x, p=2, dim=1)
\end{lstlisting}

\subsection{Training Infrastructure}

\subsubsection{Triplet Mining Strategy}

We implemented online hard triplet mining for efficient training:

\begin{lstlisting}[language=Python, caption=Online Hard Triplet Mining]
class HardTripletMiner:
    def __init__(self, margin=0.3):
        self.margin = margin

    def mine_triplets(self, embeddings, labels):
        """Mine hard triplets from batch"""
        batch_size = embeddings.size(0)

        # Compute pairwise distances
        distances = torch.cdist(embeddings, embeddings, p=2)

        # Create masks for positive and negative pairs
        pos_mask = labels.unsqueeze(0) == labels.unsqueeze(1)
        neg_mask = labels.unsqueeze(0) != labels.unsqueeze(1)

        # Mine hard positives (farthest positive for each anchor)
        pos_distances = distances * pos_mask.float()
        pos_distances[pos_distances == 0] = float('-inf')
        hard_pos_idx = torch.argmax(pos_distances, dim=1)

        # Mine hard negatives (closest negative for each anchor)
        neg_distances = distances.clone()
        neg_distances[~neg_mask] = float('inf')
        hard_neg_idx = torch.argmin(neg_distances, dim=1)

        return hard_pos_idx, hard_neg_idx

class TripletLoss(nn.Module):
    def __init__(self, margin=0.3):
        super(TripletLoss, self).__init__()
        self.margin = margin

    def forward(self, anchor, positive, negative):
        pos_dist = F.pairwise_distance(anchor, positive, p=2)
        neg_dist = F.pairwise_distance(anchor, negative, p=2)

        loss = F.relu(pos_dist - neg_dist + self.margin)
        return loss.mean()
\end{lstlisting}

\subsubsection{Training Loop Implementation}

\begin{lstlisting}[language=Python, caption=Training Loop with Progressive Unfreezing]
def train_model(model, train_loader, val_loader, num_epochs=100):
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)
    triplet_loss = TripletLoss(margin=0.3)
    triplet_miner = HardTripletMiner()

    for epoch in range(num_epochs):
        model.train()

        # Unfreeze backbone after epoch 10
        if epoch == 10 and hasattr(model, 'backbone'):
            for param in model.backbone.parameters():
                param.requires_grad = True

        total_loss = 0
        for batch_idx, (data, labels) in enumerate(train_loader):
            optimizer.zero_grad()

            # Forward pass
            embeddings = model(data)

            # Mine hard triplets
            pos_idx, neg_idx = triplet_miner.mine_triplets(embeddings, labels)

            # Compute triplet loss
            anchors = embeddings
            positives = embeddings[pos_idx]
            negatives = embeddings[neg_idx]

            loss = triplet_loss(anchors, positives, negatives)

            # Backward pass
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        scheduler.step()

        # Validation
        if epoch % 5 == 0:
            val_loss = evaluate_model(model, val_loader, triplet_loss)
            print(f'Epoch {epoch}: Train Loss: {total_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}')
\end{lstlisting}

\subsection{Evaluation Framework}

We implemented comprehensive evaluation metrics to assess model performance:

\begin{lstlisting}[language=Python, caption=Evaluation Metrics Implementation]
class EvaluationFramework:
    def __init__(self, model, test_loader):
        self.model = model
        self.test_loader = test_loader

    def compute_triplet_accuracy(self):
        """Compute percentage of correctly ordered triplets"""
        self.model.eval()
        correct = 0
        total = 0

        with torch.no_grad():
            for data, labels in self.test_loader:
                embeddings = self.model(data)

                # Generate all possible triplets
                for i in range(len(embeddings)):
                    anchor = embeddings[i]
                    anchor_label = labels[i]

                    # Find positive and negative examples
                    pos_mask = (labels == anchor_label) & (torch.arange(len(labels)) != i)
                    neg_mask = (labels != anchor_label)

                    if pos_mask.sum() > 0 and neg_mask.sum() > 0:
                        pos_distances = torch.norm(embeddings[pos_mask] - anchor, dim=1)
                        neg_distances = torch.norm(embeddings[neg_mask] - anchor, dim=1)

                        min_pos = pos_distances.min()
                        min_neg = neg_distances.min()

                        if min_pos < min_neg:
                            correct += 1
                        total += 1

        return correct / total if total > 0 else 0

    def compute_embedding_quality_metrics(self):
        """Compute various embedding quality metrics"""
        metrics = {
            'triplet_accuracy': self.compute_triplet_accuracy(),
            'silhouette_score': self._compute_silhouette_score(),
            'intra_cluster_distance': self._compute_intra_cluster_distance(),
            'inter_cluster_distance': self._compute_inter_cluster_distance()
        }
        return metrics
\end{lstlisting}

\subsection{Computational Infrastructure}

\textbf{Hardware Specifications}:
\begin{itemize}
    \item GPU: NVIDIA RTX 3080/4090 for model training
    \item CPU: Intel i7-10700K / AMD Ryzen 7 3700X
    \item RAM: 32GB DDR4
    \item Storage: 1TB NVMe SSD for fast data access
\end{itemize}

\textbf{Distributed Training}: For large-scale experiments, we utilized PyTorch's DistributedDataParallel for multi-GPU training to reduce training time.

\textbf{Experiment Tracking}: We used Weights & Biases (wandb) for experiment tracking, hyperparameter tuning, and result visualization.
