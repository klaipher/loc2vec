\section{Related Work}
\label{sec:related_work}

This section reviews the foundational work and recent advances in spatial representation learning, embedding techniques, and their applications to geographical data.

\subsection{Word Embeddings and Representation Learning}

The concept of learning dense vector representations was popularized in natural language processing with Word2Vec \cite{mikolov2013word2vec}, which demonstrated that meaningful semantic relationships could be captured in vector spaces. The key insight was that words appearing in similar contexts should have similar representations, leading to the famous example "king - man + woman = queen" in the embedding space.

This paradigm inspired researchers to apply similar techniques to other domains, including spatial data. The fundamental principle remains the same: entities with similar properties or contexts should be positioned closer together in the learned embedding space.

\subsection{Spatial Representation Learning}

\subsubsection{Traditional Approaches}

Early approaches to spatial data representation relied heavily on hand-crafted features and geographical coordinates. Methods such as:

\begin{itemize}
    \item \textbf{Coordinate-based features:} Using latitude and longitude directly as features
    \item \textbf{Distance metrics:} Calculating distances to points of interest or landmarks
    \item \textbf{Grid-based representations:} Discretizing space into regular grids
    \item \textbf{Administrative boundaries:} Using political or administrative divisions as categorical features
\end{itemize}

While these approaches provide basic spatial information, they fail to capture the rich semantic relationships between locations and are often insufficient for complex spatial reasoning tasks.

\subsubsection{Deep Learning for Spatial Data}

Recent advances in deep learning have enabled more sophisticated approaches to spatial representation:

\textbf{Place2Vec} \cite{place2vec} adapted Word2Vec principles to learn representations of places based on check-in sequences from location-based social networks. The method treats sequences of visited places as sentences and learns embeddings that capture both geographical proximity and semantic similarity.

\textbf{Geo-Embeddings} \cite{geo2vec} focused on learning embeddings for administrative regions using various geographical and socio-economic features. This work demonstrated how embeddings could capture complex relationships between different geographical units.

\textbf{Spatial2Vec} \cite{spatial2vec} introduced a framework for learning spatial embeddings from mobility data, showing how movement patterns could inform meaningful location representations.

\textbf{LocVec} \cite{hal2019locvec} proposed a novel approach for similar location recommendation based on geotagged social media posts, demonstrating how user-generated content can be leveraged to understand location semantics and similarity patterns.

\subsection{Triplet Loss and Metric Learning}

Triplet loss, introduced by \cite{schroff2015facenet} for face recognition, has become a powerful tool for learning embeddings in metric spaces. The loss function operates on triplets of examples:

\begin{equation}
L = \max(0, ||f(a) - f(p)||^2 - ||f(a) - f(n)||^2 + \alpha)
\end{equation}

where $f(a)$, $f(p)$, and $f(n)$ are embeddings of anchor, positive, and negative examples respectively, and $\alpha$ is the margin.

This approach directly optimizes the embedding space to ensure that similar examples are closer than dissimilar ones, making it particularly suitable for learning meaningful representations where the notion of similarity is crucial.

\subsection{Convolutional Neural Networks for Spatial Data}

CNNs have proven effective for processing spatial data in various forms:

\textbf{Satellite Imagery Analysis:} Works like \cite{jean2016combining} demonstrated how CNNs could extract meaningful features from satellite images for socio-economic prediction tasks.

\textbf{Map Tile Processing:} \cite{zhou2018deeplearning} showed how map tiles could be processed using CNNs to understand urban structures and land use patterns.

\textbf{Multi-modal Spatial Learning:} Recent approaches combine multiple data sources (images, maps, metadata) to learn richer spatial representations \cite{multimodal2020}.

\subsection{Transfer Learning in Computer Vision}

Transfer learning has revolutionized computer vision by enabling the use of pre-trained models on new tasks:

\textbf{ImageNet Pre-training:} Models trained on ImageNet \cite{imagenet} have shown remarkable transfer capabilities across diverse visual tasks.

\textbf{Architecture Evolution:} Modern architectures like ResNet \cite{he2016resnet}, EfficientNet \cite{tan2019efficientnet}, and MobileNet \cite{howard2017mobilenets} have improved both accuracy and efficiency, making them attractive for transfer learning scenarios.

\textbf{Domain Adaptation:} Techniques for adapting pre-trained models to new domains have been extensively studied, with applications ranging from medical imaging to satellite data analysis.

\subsection{The Original Loc2Vec Approach}

The Loc2Vec methodology \cite{sentiance2018loc2vec} combines several of these concepts:

\begin{itemize}
    \item \textbf{Map Rasterization:} Converting OpenStreetMap data into multi-channel tensors
    \item \textbf{Triplet Loss Training:} Using spatial and semantic similarity to define positive and negative pairs
    \item \textbf{CNN Encoding:} Employing convolutional networks to process rasterized map data
    \item \textbf{Self-supervised Learning:} Learning without manually labeled data by using geographical relationships
\end{itemize}

The approach demonstrated that meaningful location embeddings could be learned by treating geographical similarity and proximity as supervision signals for the learning process.

\subsection{Gaps and Opportunities}

While previous work has shown promising results, several opportunities for improvement exist:

\begin{itemize}
    \item \textbf{Architecture Modernization:} Most existing approaches use older CNN architectures
    \item \textbf{Transfer Learning Integration:} Limited exploration of how modern pre-trained models can benefit spatial embedding learning
    \item \textbf{City-specific Adaptation:} Most approaches are designed for general use rather than specific urban contexts
    \item \textbf{Evaluation Methodology:} Lack of standardized evaluation frameworks for spatial embeddings
\end{itemize}

Our work addresses these gaps by implementing a modern version of Loc2Vec with contemporary neural architectures and transfer learning techniques, specifically adapted for Kyiv city.
