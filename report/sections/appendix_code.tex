\subsection{Data Processing Pipeline}

\subsubsection{OSM Data Extraction and Processing}

\begin{lstlisting}[language=Python, caption=Complete OSM Data Processing Pipeline]
import osmium
import psycopg2
from sqlalchemy import create_engine
import geopandas as gpd
from shapely.geometry import box

class OSMProcessor:
    def __init__(self, db_config):
        self.engine = create_engine(f"postgresql://{db_config['user']}:"
                                  f"{db_config['password']}@{db_config['host']}/"
                                  f"{db_config['database']}")

    def setup_kyiv_database(self):
        """Set up PostGIS database with Kyiv OSM data"""
        # Define Kyiv bounding box
        kyiv_bounds = {
            'north': 50.6195,
            'south': 50.2134,
            'east': 30.8251,
            'west': 30.0961
        }

        # Create spatial tables
        self.create_spatial_tables()

        # Import OSM data
        self.import_osm_data(kyiv_bounds)

        # Create spatial indices
        self.create_spatial_indices()

    def create_spatial_tables(self):
        """Create spatial tables for different OSM features"""
        tables = {
            'roads': '''
                CREATE TABLE IF NOT EXISTS roads (
                    id SERIAL PRIMARY KEY,
                    osm_id BIGINT,
                    highway VARCHAR(50),
                    name VARCHAR(200),
                    geom GEOMETRY(LINESTRING, 4326)
                );
            ''',
            'buildings': '''
                CREATE TABLE IF NOT EXISTS buildings (
                    id SERIAL PRIMARY KEY,
                    osm_id BIGINT,
                    building VARCHAR(50),
                    name VARCHAR(200),
                    geom GEOMETRY(POLYGON, 4326)
                );
            ''',
            'landuse': '''
                CREATE TABLE IF NOT EXISTS landuse (
                    id SERIAL PRIMARY KEY,
                    osm_id BIGINT,
                    landuse VARCHAR(50),
                    leisure VARCHAR(50),
                    natural VARCHAR(50),
                    geom GEOMETRY(POLYGON, 4326)
                );
            '''
        }

        with self.engine.connect() as conn:
            for table, sql in tables.items():
                conn.execute(sql)

    def extract_features_for_location(self, lat, lon, radius_m=500):
        """Extract geographical features for a given location"""
        query = f"""
        WITH location AS (
            SELECT ST_Transform(ST_GeomFromText('POINT({lon} {lat})', 4326), 3857) as geom
        ),
        buffer AS (
            SELECT ST_Buffer(location.geom, {radius_m}) as geom FROM location
        )
        SELECT
            'road' as feature_type,
            highway as category,
            ST_AsText(ST_Transform(r.geom, 4326)) as geometry
        FROM roads r, buffer b
        WHERE ST_Intersects(ST_Transform(r.geom, 3857), b.geom)
        UNION ALL
        SELECT
            'building' as feature_type,
            building as category,
            ST_AsText(ST_Transform(bldg.geom, 4326)) as geometry
        FROM buildings bldg, buffer b
        WHERE ST_Intersects(ST_Transform(bldg.geom, 3857), b.geom)
        UNION ALL
        SELECT
            'landuse' as feature_type,
            COALESCE(landuse, leisure, natural) as category,
            ST_AsText(ST_Transform(lu.geom, 4326)) as geometry
        FROM landuse lu, buffer b
        WHERE ST_Intersects(ST_Transform(lu.geom, 3857), b.geom)
        """

        return pd.read_sql(query, self.engine)
\end{lstlisting}

\subsubsection{Multi-channel Rasterization Implementation}

\begin{lstlisting}[language=Python, caption=Comprehensive Rasterization System]
import mapnik
import numpy as np
from PIL import Image
import io

class AdvancedKyivRasterizer:
    def __init__(self, db_connection, tile_size=256):
        self.db_connection = db_connection
        self.tile_size = tile_size
        self.channel_configs = self._initialize_channel_configs()
        self.style_cache = {}

    def _initialize_channel_configs(self):
        """Define styling for each geographical channel"""
        return {
            'roads_primary': {
                'layer_sql': """
                    SELECT geom FROM roads
                    WHERE highway IN ('primary', 'trunk', 'motorway')
                """,
                'symbolizer': {
                    'stroke': '#FF0000',
                    'stroke_width': 3,
                    'stroke_opacity': 1.0
                }
            },
            'roads_secondary': {
                'layer_sql': """
                    SELECT geom FROM roads
                    WHERE highway IN ('secondary', 'tertiary')
                """,
                'symbolizer': {
                    'stroke': '#00FF00',
                    'stroke_width': 2,
                    'stroke_opacity': 1.0
                }
            },
            'roads_local': {
                'layer_sql': """
                    SELECT geom FROM roads
                    WHERE highway IN ('residential', 'service', 'unclassified')
                """,
                'symbolizer': {
                    'stroke': '#0000FF',
                    'stroke_width': 1,
                    'stroke_opacity': 1.0
                }
            },
            'buildings': {
                'layer_sql': "SELECT geom FROM buildings",
                'symbolizer': {
                    'fill': '#888888',
                    'fill_opacity': 1.0,
                    'stroke': '#000000',
                    'stroke_width': 0.5
                }
            },
            'water': {
                'layer_sql': """
                    SELECT geom FROM landuse
                    WHERE natural = 'water' OR landuse = 'reservoir'
                """,
                'symbolizer': {
                    'fill': '#0080FF',
                    'fill_opacity': 1.0
                }
            },
            'green_spaces': {
                'layer_sql': """
                    SELECT geom FROM landuse
                    WHERE leisure IN ('park', 'garden') OR natural IN ('forest', 'wood')
                """,
                'symbolizer': {
                    'fill': '#00FF80',
                    'fill_opacity': 1.0
                }
            },
            'commercial': {
                'layer_sql': """
                    SELECT geom FROM landuse
                    WHERE landuse IN ('commercial', 'retail')
                """,
                'symbolizer': {
                    'fill': '#FF8000',
                    'fill_opacity': 1.0
                }
            },
            'residential': {
                'layer_sql': """
                    SELECT geom FROM landuse
                    WHERE landuse = 'residential'
                """,
                'symbolizer': {
                    'fill': '#FFFF00',
                    'fill_opacity': 1.0
                }
            },
            'industrial': {
                'layer_sql': """
                    SELECT geom FROM landuse
                    WHERE landuse = 'industrial'
                """,
                'symbolizer': {
                    'fill': '#800080',
                    'fill_opacity': 1.0
                }
            }
        }

    def generate_multichannel_tile(self, lat, lon, radius_m=500,
                                  augment=None):
        """Generate multi-channel raster tile with optional augmentation"""
        # Calculate bounding box
        bbox = self._calculate_bbox(lat, lon, radius_m)

        # Apply augmentation to bbox if specified
        if augment:
            bbox = self._apply_augmentation(bbox, augment)

        # Initialize output tensor
        tile = np.zeros((self.tile_size, self.tile_size, 12), dtype=np.float32)

        # Render each channel
        for i, (channel_name, config) in enumerate(self.channel_configs.items()):
            if i >= 12:  # Limit to 12 channels
                break

            channel_data = self._render_channel(bbox, config)
            tile[:, :, i] = channel_data

        return tile

    def _render_channel(self, bbox, config):
        """Render a single geographical channel"""
        # Create Mapnik map
        m = mapnik.Map(self.tile_size, self.tile_size)

        # Create layer
        layer = mapnik.Layer('layer')
        layer.datasource = mapnik.PostGIS(
            host=self.db_connection['host'],
            user=self.db_connection['user'],
            password=self.db_connection['password'],
            dbname=self.db_connection['database'],
            table=f"({config['layer_sql']}) as data",
            geometry_field='geom',
            srid=4326
        )

        # Create style
        style = mapnik.Style()
        rule = mapnik.Rule()

        if 'stroke' in config['symbolizer']:
            # Line symbolizer for roads
            line_sym = mapnik.LineSymbolizer()
            line_sym.stroke = mapnik.Color(config['symbolizer']['stroke'])
            line_sym.stroke_width = config['symbolizer']['stroke_width']
            rule.symbols.append(line_sym)

        if 'fill' in config['symbolizer']:
            # Polygon symbolizer for areas
            poly_sym = mapnik.PolygonSymbolizer()
            poly_sym.fill = mapnik.Color(config['symbolizer']['fill'])
            rule.symbols.append(poly_sym)

        style.rules.append(rule)
        m.append_style('style', style)
        layer.styles.append('style')
        m.layers.append(layer)

        # Set projection and zoom
        m.srs = '+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs'
        m.zoom_to_box(mapnik.Box2d(*bbox))

        # Render to image
        im = mapnik.Image(self.tile_size, self.tile_size)
        mapnik.render(m, im)

        # Convert to numpy array (binary mask)
        img_data = np.frombuffer(im.tostring('png'), dtype=np.uint8)
        pil_img = Image.open(io.BytesIO(img_data))

        # Convert to grayscale and normalize
        grayscale = np.array(pil_img.convert('L'))
        return (grayscale > 0).astype(np.float32)

    def _apply_augmentation(self, bbox, augment):
        """Apply spatial augmentation to bounding box"""
        if 'rotation' in augment:
            # Implement rotation logic
            pass

        if 'translation' in augment:
            dx = augment['translation']['x']
            dy = augment['translation']['y']
            bbox = [bbox[0] + dx, bbox[1] + dy,
                    bbox[2] + dx, bbox[3] + dy]

        if 'scale' in augment:
            # Implement scaling logic
            pass

        return bbox
\end{lstlisting}

\subsection{Model Architecture Implementations}

\subsubsection{Enhanced Base CNN with Attention}

\begin{lstlisting}[language=Python, caption=Enhanced Base CNN Architecture]
import torch
import torch.nn as nn
import torch.nn.functional as F

class SpatialAttentionModule(nn.Module):
    def __init__(self, in_channels):
        super(SpatialAttentionModule, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, in_channels // 8, 1)
        self.conv2 = nn.Conv2d(in_channels // 8, 1, 1)

    def forward(self, x):
        # Spatial attention
        att = self.conv1(x)
        att = F.relu(att)
        att = self.conv2(att)
        att = torch.sigmoid(att)

        return x * att

class ChannelAttentionModule(nn.Module):
    def __init__(self, in_channels, reduction=16):
        super(ChannelAttentionModule, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)

        self.fc = nn.Sequential(
            nn.Linear(in_channels, in_channels // reduction),
            nn.ReLU(),
            nn.Linear(in_channels // reduction, in_channels)
        )

    def forward(self, x):
        b, c, h, w = x.size()

        # Average pooling branch
        avg_out = self.avg_pool(x).view(b, c)
        avg_out = self.fc(avg_out).view(b, c, 1, 1)

        # Max pooling branch
        max_out = self.max_pool(x).view(b, c)
        max_out = self.fc(max_out).view(b, c, 1, 1)

        # Combine and apply attention
        att = torch.sigmoid(avg_out + max_out)
        return x * att

class EnhancedLoc2VecEncoder(nn.Module):
    def __init__(self, embedding_dim=256, use_attention=True):
        super(EnhancedLoc2VecEncoder, self).__init__()
        self.use_attention = use_attention

        # Input processing
        self.input_norm = nn.BatchNorm2d(12)

        # Convolutional blocks with residual connections
        self.block1 = self._make_conv_block(12, 64, 2)
        self.block2 = self._make_conv_block(64, 128, 2)
        self.block3 = self._make_conv_block(128, 256, 2)
        self.block4 = self._make_conv_block(256, 512, 2)

        # Attention modules
        if use_attention:
            self.spatial_att3 = SpatialAttentionModule(256)
            self.channel_att4 = ChannelAttentionModule(512)

        # Global feature extraction
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        self.global_max_pool = nn.AdaptiveMaxPool2d(1)

        # Classification head
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(512 * 2, 1024),
            nn.BatchNorm1d(1024),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(1024, embedding_dim)
        )

    def _make_conv_block(self, in_channels, out_channels, num_layers):
        layers = []

        # First conv with potential channel change
        layers.append(nn.Conv2d(in_channels, out_channels, 3, padding=1))
        layers.append(nn.BatchNorm2d(out_channels))
        layers.append(nn.ReLU(inplace=True))

        # Additional conv layers
        for _ in range(num_layers - 1):
            layers.append(nn.Conv2d(out_channels, out_channels, 3, padding=1))
            layers.append(nn.BatchNorm2d(out_channels))
            layers.append(nn.ReLU(inplace=True))

        # Max pooling
        layers.append(nn.MaxPool2d(2, 2))

        return nn.Sequential(*layers)

    def forward(self, x):
        # Input normalization
        x = self.input_norm(x)

        # Convolutional blocks
        x1 = self.block1(x)      # 64 channels
        x2 = self.block2(x1)     # 128 channels
        x3 = self.block3(x2)     # 256 channels

        # Apply spatial attention to block 3
        if self.use_attention:
            x3 = self.spatial_att3(x3)

        x4 = self.block4(x3)     # 512 channels

        # Apply channel attention to block 4
        if self.use_attention:
            x4 = self.channel_att4(x4)

        # Global pooling
        avg_pool = self.global_pool(x4).view(x4.size(0), -1)
        max_pool = self.global_max_pool(x4).view(x4.size(0), -1)

        # Concatenate pooled features
        features = torch.cat([avg_pool, max_pool], dim=1)

        # Classification
        embedding = self.classifier(features)

        # L2 normalization
        embedding = F.normalize(embedding, p=2, dim=1)

        return embedding
\end{lstlisting}

\subsection{Training and Evaluation Utilities}

\subsubsection{Advanced Triplet Mining and Loss}

\begin{lstlisting}[language=Python, caption=Advanced Triplet Mining Implementation]
import torch
import torch.nn as nn
import numpy as np

class AdvancedTripletMiner:
    def __init__(self, margin=0.3, mining_strategy='hard'):
        self.margin = margin
        self.mining_strategy = mining_strategy

    def mine_triplets(self, embeddings, labels, epoch=None):
        """Mine triplets using various strategies"""
        if self.mining_strategy == 'hard':
            return self._hard_mining(embeddings, labels)
        elif self.mining_strategy == 'semi_hard':
            return self._semi_hard_mining(embeddings, labels)
        elif self.mining_strategy == 'curriculum':
            return self._curriculum_mining(embeddings, labels, epoch)
        else:
            return self._random_mining(embeddings, labels)

    def _hard_mining(self, embeddings, labels):
        """Hard negative mining: hardest positive and negative for each anchor"""
        batch_size = embeddings.size(0)
        distances = torch.cdist(embeddings, embeddings, p=2)

        # Create masks
        pos_mask = labels.unsqueeze(0) == labels.unsqueeze(1)
        neg_mask = labels.unsqueeze(0) != labels.unsqueeze(1)

        # Remove diagonal (self-distances)
        pos_mask.fill_diagonal_(False)

        triplets = []
        for i in range(batch_size):
            # Find valid positives and negatives
            pos_indices = torch.where(pos_mask[i])[0]
            neg_indices = torch.where(neg_mask[i])[0]

            if len(pos_indices) > 0 and len(neg_indices) > 0:
                # Hard positive (farthest positive)
                pos_distances = distances[i][pos_indices]
                hard_pos_idx = pos_indices[torch.argmax(pos_distances)]

                # Hard negative (closest negative)
                neg_distances = distances[i][neg_indices]
                hard_neg_idx = neg_indices[torch.argmin(neg_distances)]

                triplets.append((i, hard_pos_idx.item(), hard_neg_idx.item()))

        return triplets

    def _semi_hard_mining(self, embeddings, labels):
        """Semi-hard mining: negatives that are farther than positive but within margin"""
        batch_size = embeddings.size(0)
        distances = torch.cdist(embeddings, embeddings, p=2)

        triplets = []
        for i in range(batch_size):
            pos_mask = (labels == labels[i]) & (torch.arange(batch_size) != i)
            neg_mask = (labels != labels[i])

            pos_indices = torch.where(pos_mask)[0]
            neg_indices = torch.where(neg_mask)[0]

            if len(pos_indices) > 0 and len(neg_indices) > 0:
                # Random positive
                pos_idx = pos_indices[torch.randint(0, len(pos_indices), (1,))].item()
                pos_dist = distances[i][pos_idx]

                # Semi-hard negative
                neg_distances = distances[i][neg_indices]
                semi_hard_mask = (neg_distances > pos_dist) & \
                               (neg_distances < pos_dist + self.margin)

                if semi_hard_mask.sum() > 0:
                    semi_hard_indices = neg_indices[semi_hard_mask]
                    neg_idx = semi_hard_indices[torch.randint(0, len(semi_hard_indices), (1,))].item()
                else:
                    # Fall back to hard negative
                    neg_idx = neg_indices[torch.argmin(neg_distances)].item()

                triplets.append((i, pos_idx, neg_idx))

        return triplets

class FocalTripletLoss(nn.Module):
    def __init__(self, margin=0.3, alpha=1.0, gamma=2.0):
        super(FocalTripletLoss, self).__init__()
        self.margin = margin
        self.alpha = alpha
        self.gamma = gamma

    def forward(self, anchor, positive, negative):
        pos_dist = F.pairwise_distance(anchor, positive, p=2)
        neg_dist = F.pairwise_distance(anchor, negative, p=2)

        # Standard triplet loss
        basic_loss = F.relu(pos_dist - neg_dist + self.margin)

        # Focal weight: focus on hard examples
        pt = torch.exp(-basic_loss)
        focal_weight = self.alpha * (1 - pt) ** self.gamma

        focal_loss = focal_weight * basic_loss
        return focal_loss.mean()

class CurriculumTripletLoss(nn.Module):
    def __init__(self, margin=0.3, curriculum_epochs=20):
        super(CurriculumTripletLoss, self).__init__()
        self.margin = margin
        self.curriculum_epochs = curriculum_epochs

    def forward(self, anchor, positive, negative, epoch=0):
        pos_dist = F.pairwise_distance(anchor, positive, p=2)
        neg_dist = F.pairwise_distance(anchor, negative, p=2)

        # Curriculum margin: start with larger margin, decrease over time
        curriculum_margin = self.margin * (1 + np.exp(-epoch / self.curriculum_epochs))

        loss = F.relu(pos_dist - neg_dist + curriculum_margin)
        return loss.mean()
\end{lstlisting}

\subsection{Comprehensive Evaluation Framework}

\subsubsection{Advanced Metrics and Visualization}

\begin{lstlisting}[language=Python, caption=Comprehensive Evaluation Implementation]
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from sklearn.manifold import TSNE
import umap

class ComprehensiveEvaluator:
    def __init__(self, model, device='cuda'):
        self.model = model
        self.device = device

    def evaluate_comprehensive(self, test_loader, venue_data=None):
        """Comprehensive evaluation including all metrics"""
        results = {}

        # Extract embeddings and labels
        embeddings, labels = self._extract_embeddings(test_loader)

        # Embedding quality metrics
        results['embedding_quality'] = self._evaluate_embedding_quality(embeddings, labels)

        # Venue mapping evaluation
        if venue_data is not None:
            results['venue_mapping'] = self._evaluate_venue_mapping(embeddings, venue_data)

        # Visualization
        results['visualizations'] = self._create_visualizations(embeddings, labels)

        # Computational metrics
        results['computational'] = self._evaluate_computational_performance(test_loader)

        return results

    def _evaluate_embedding_quality(self, embeddings, labels):
        """Evaluate quality of learned embeddings"""
        metrics = {}

        # Clustering metrics
        metrics['silhouette_score'] = silhouette_score(embeddings, labels)
        metrics['calinski_harabasz'] = calinski_harabasz_score(embeddings, labels)

        # Distance-based metrics
        metrics.update(self._compute_distance_metrics(embeddings, labels))

        # Triplet accuracy
        metrics['triplet_accuracy'] = self._compute_triplet_accuracy(embeddings, labels)

        return metrics

    def _compute_distance_metrics(self, embeddings, labels):
        """Compute various distance-based metrics"""
        metrics = {}

        # Compute pairwise distances
        distances = np.linalg.norm(embeddings[:, None] - embeddings[None, :], axis=2)

        # Intra-class distances
        intra_distances = []
        for label in np.unique(labels):
            mask = labels == label
            if mask.sum() > 1:
                class_distances = distances[mask][:, mask]
                intra_distances.extend(class_distances[np.triu_indices_from(class_distances, k=1)])

        # Inter-class distances
        inter_distances = []
        for i, label1 in enumerate(np.unique(labels)):
            for j, label2 in enumerate(np.unique(labels)):
                if i < j:
                    mask1 = labels == label1
                    mask2 = labels == label2
                    inter_distances.extend(distances[mask1][:, mask2].flatten())

        metrics['intra_class_distance'] = np.mean(intra_distances)
        metrics['inter_class_distance'] = np.mean(inter_distances)
        metrics['distance_ratio'] = np.mean(inter_distances) / np.mean(intra_distances)

        return metrics

    def _create_visualizations(self, embeddings, labels):
        """Create various embedding visualizations"""
        visualizations = {}

        # t-SNE visualization
        tsne = TSNE(n_components=2, random_state=42, perplexity=30)
        tsne_result = tsne.fit_transform(embeddings)

        fig, ax = plt.subplots(figsize=(10, 8))
        scatter = ax.scatter(tsne_result[:, 0], tsne_result[:, 1],
                           c=labels, cmap='tab10', alpha=0.7)
        ax.set_title('t-SNE Visualization of Location Embeddings')
        plt.colorbar(scatter)
        visualizations['tsne'] = fig

        # UMAP visualization
        umap_reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)
        umap_result = umap_reducer.fit_transform(embeddings)

        fig, ax = plt.subplots(figsize=(10, 8))
        scatter = ax.scatter(umap_result[:, 0], umap_result[:, 1],
                           c=labels, cmap='tab10', alpha=0.7)
        ax.set_title('UMAP Visualization of Location Embeddings')
        plt.colorbar(scatter)
        visualizations['umap'] = fig

        # Distance distribution
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))

        # Intra-class distances
        intra_distances = self._get_intra_class_distances(embeddings, labels)
        axes[0].hist(intra_distances, bins=50, alpha=0.7, label='Intra-class')
        axes[0].set_title('Intra-class Distance Distribution')
        axes[0].set_xlabel('Distance')
        axes[0].set_ylabel('Frequency')

        # Inter-class distances
        inter_distances = self._get_inter_class_distances(embeddings, labels)
        axes[1].hist(inter_distances, bins=50, alpha=0.7, label='Inter-class')
        axes[1].set_title('Inter-class Distance Distribution')
        axes[1].set_xlabel('Distance')
        axes[1].set_ylabel('Frequency')

        visualizations['distance_distributions'] = fig

        return visualizations

    def _evaluate_computational_performance(self, test_loader):
        """Evaluate computational performance metrics"""
        import time

        metrics = {}

        # Inference time
        self.model.eval()
        times = []

        with torch.no_grad():
            for i, (data, _) in enumerate(test_loader):
                if i >= 100:  # Test on first 100 batches
                    break

                data = data.to(self.device)

                start_time = time.time()
                _ = self.model(data)
                end_time = time.time()

                times.append(end_time - start_time)

        metrics['avg_inference_time'] = np.mean(times)
        metrics['std_inference_time'] = np.std(times)

        # Memory usage
        if torch.cuda.is_available():
            metrics['gpu_memory_mb'] = torch.cuda.max_memory_allocated() / 1024 / 1024

        # Model size
        param_count = sum(p.numel() for p in self.model.parameters())
        metrics['parameter_count'] = param_count
        metrics['model_size_mb'] = param_count * 4 / 1024 / 1024  # Assuming float32

        return metrics
\end{lstlisting}
