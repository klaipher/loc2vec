\subsection{Original Loc2Vec Methodology}

The Loc2Vec approach, as proposed by Sentiance, addresses venue mapping by learning distributed representations of geographical locations. The core methodology consists of three main components:

\subsubsection{Geographic Data Rasterization}

Instead of working directly with raw geographical coordinates, Loc2Vec transforms spatial data into multi-channel raster representations. For a given location coordinate $(lat, lon)$ and radius $r$, the system:

\begin{enumerate}
    \item Queries a spatial database (PostGIS with OpenStreetMap data) to retrieve geographical features within the specified radius
    \item Categorizes features into semantic layers (roads, buildings, parks, water bodies, etc.)
    \item Rasterizes each category into separate channels using Mapnik rendering engine
    \item Produces a 12-channel tensor where each channel contains specific geographical information
\end{enumerate}

The 12 channels typically include:
\begin{itemize}
    \item Road networks (primary, secondary, residential)
    \item Building footprints
    \item Land use categories (residential, commercial, industrial)
    \item Natural features (water bodies, parks, forests)
    \item Amenities and points of interest
    \item Transportation infrastructure
\end{itemize}

\subsubsection{Triplet Network Architecture}

The embedding learning process uses a triplet network architecture with the following components:

\textbf{Encoder Network}: A convolutional neural network $f_\theta: \mathbb{R}^{H \times W \times 12} \rightarrow \mathbb{R}^d$ that maps the 12-channel raster representation to a $d$-dimensional embedding vector.

\textbf{Triplet Loss Function}: The network is trained using triplet loss to ensure that semantically similar locations have similar embeddings:

\begin{equation}
L = \max(0, \|f_\theta(x_a) - f_\theta(x_p)\|_2^2 - \|f_\theta(x_a) - f_\theta(x_n)\|_2^2 + \alpha)
\end{equation}

where:
\begin{itemize}
    \item $x_a$ is the anchor location
    \item $x_p$ is a positive example (semantically similar location)
    \item $x_n$ is a negative example (semantically dissimilar location)
    \item $\alpha$ is the margin parameter
\end{itemize}

\subsubsection{Triplet Generation Strategy}

Effective triplet generation is crucial for learning meaningful embeddings. The original approach uses:

\textbf{Spatial similarity}: Locations within a small radius are considered positive pairs, while distant locations serve as negative examples.

\textbf{Semantic similarity}: Areas with similar land use patterns or feature distributions are grouped as positive pairs.

\textbf{Hard negative mining}: Dynamically selecting challenging negative examples during training to improve learning efficiency.

\subsection{Our Adaptations for Kyiv City}

We adapted the original methodology to work effectively with Kyiv's geographical characteristics and data availability:

\subsubsection{Data Source and Processing}

\textbf{OpenStreetMap Integration}: We established a pipeline to extract and process OSM data specifically for Kyiv region, including:
\begin{itemize}
    \item Setting up local PostGIS database with Ukraine OSM data
    \item Configuring spatial indices for efficient querying
    \item Implementing data validation and cleaning procedures
\end{itemize}

\textbf{Ukrainian Geographic Features}: We adapted the feature categorization to capture characteristics specific to Kyiv:
\begin{itemize}
    \item Soviet-era urban planning patterns
    \item Dnieper River and waterfront areas
    \item Historical districts with unique architectural styles
    \item Modern commercial developments
\end{itemize}

\subsubsection{Rasterization Pipeline}

We implemented a flexible rasterization system with the following components:

\textbf{Multi-scale Processing}: Generate tiles at different zoom levels to capture both local detail and broader context.

\textbf{Data Augmentation}: Implement rotation, translation, and scaling augmentations to improve model robustness:
\begin{itemize}
    \item Random rotation: $[-45°, 45°]$
    \item Translation: up to 20\% of tile size
    \item Scale variation: $[0.8, 1.2]$ factor
\end{itemize}

\textbf{Channel Optimization}: Customize the 12-channel representation based on Kyiv's geographical characteristics and OSM data availability.

\subsubsection{Embedding Architecture}

Our base encoder architecture follows the original design but with modifications for improved performance:

\begin{algorithm}
\caption{Base CNN Encoder Architecture}
\begin{algorithmic}
\STATE Input: $x \in \mathbb{R}^{256 \times 256 \times 12}$
\STATE Conv2D(32, 3×3, stride=1, padding=1) + ReLU
\STATE Conv2D(32, 3×3, stride=1, padding=1) + ReLU
\STATE MaxPool2D(2×2)
\STATE Conv2D(64, 3×3, stride=1, padding=1) + ReLU
\STATE Conv2D(64, 3×3, stride=1, padding=1) + ReLU
\STATE MaxPool2D(2×2)
\STATE Conv2D(128, 3×3, stride=1, padding=1) + ReLU
\STATE Conv2D(128, 3×3, stride=1, padding=1) + ReLU
\STATE MaxPool2D(2×2)
\STATE GlobalAveragePooling2D()
\STATE Dense(512) + ReLU
\STATE Dense(256) + L2-Normalization
\STATE Output: $embedding \in \mathbb{R}^{256}$
\end{algorithmic}
\end{algorithm}

\subsection{Transfer Learning Methodology}

Beyond the base implementation, we systematically evaluated transfer learning approaches using three state-of-the-art CNN architectures:

\subsubsection{Architecture Selection}

We selected three representative architectures spanning different design philosophies:

\textbf{EfficientNet-B0}: Compound scaling approach optimizing accuracy-efficiency trade-offs
\textbf{ResNet-50}: Deep residual learning with skip connections
\textbf{MobileNetv3-Large}: Mobile-optimized architecture with depthwise separable convolutions

\subsubsection{Transfer Learning Strategy}

For each architecture, we implemented the following transfer learning approach:

\begin{enumerate}
    \item \textbf{Backbone Initialization}: Load ImageNet pre-trained weights for the feature extraction layers
    \item \textbf{Input Adaptation}: Modify the first layer to accept 12-channel input instead of 3-channel RGB
    \item \textbf{Output Adaptation}: Replace the classification head with our embedding projection layer
    \item \textbf{Fine-tuning Strategy}: Implement progressive unfreezing during training
\end{enumerate}

\subsubsection{Training Procedure}

Our training procedure incorporates best practices for triplet learning:

\begin{algorithm}
\caption{Training Procedure}
\begin{algorithmic}
\STATE Initialize model with pre-trained weights
\STATE Freeze backbone layers (first 10 epochs)
\STATE FOR epoch = 1 to max\_epochs:
\STATE \quad Generate triplet batch using online hard mining
\STATE \quad Forward pass: compute embeddings
\STATE \quad Compute triplet loss + L2 regularization
\STATE \quad Backward pass and optimization step
\STATE \quad IF epoch > 10: unfreeze backbone layers
\STATE \quad Evaluate on validation set every 5 epochs
\STATE END FOR
\end{algorithmic}
\end{algorithm}

\subsection{Evaluation Methodology}

We designed a comprehensive evaluation framework to assess both the quality of learned embeddings and the practical performance for venue mapping:

\subsubsection{Embedding Quality Metrics}

\textbf{Triplet Accuracy}: Percentage of triplets where $d(anchor, positive) < d(anchor, negative)$

\textbf{Embedding Visualization}: t-SNE and UMAP projections to visualize cluster formation

\textbf{Similarity Analysis}: Correlation between embedding distances and semantic similarity

\subsubsection{Venue Mapping Evaluation}

\textbf{Accuracy@k}: Percentage of cases where the correct venue appears in top-k nearest neighbors

\textbf{Mean Reciprocal Rank (MRR)}: Average reciprocal rank of the correct venue

\textbf{Distance-based Metrics}: Comparison with baseline distance-based approaches

\subsubsection{Computational Analysis}

\textbf{Training Efficiency}: Training time, memory usage, convergence rate

\textbf{Inference Performance}: Forward pass time, model size, mobile deployment feasibility

\textbf{Transfer Learning Benefits}: Comparison of transfer learning vs. training from scratch
