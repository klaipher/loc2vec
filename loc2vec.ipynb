{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a8c6dd841b32985",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T13:53:09.263364Z",
     "start_time": "2025-06-09T13:53:02.023437Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Silicon GPU (MPS)\n"
     ]
    }
   ],
   "source": [
    "# Loc2Vec: Learning Location Embeddings with Triplet Loss Networks\n",
    "# Improved implementation following the paper and modern PyTorch practices\n",
    "#\n",
    "# Key Features:\n",
    "# - Follows the original Sentiance paper architecture\n",
    "# - Supports both custom CNN and transfer learning (ResNet, DenseNet)\n",
    "# - Optimized for Kyiv tile dataset with coordinate normalization\n",
    "# - Modern PyTorch 2.0+ features (torch.compile, AMP, etc.)\n",
    "#\n",
    "# Important Notes for Apple Silicon (M1/M2) Users:\n",
    "# - torch.compile() is automatically disabled on MPS due to backend limitations\n",
    "# - Mixed precision (AMP) is disabled on MPS as it's not yet supported\n",
    "# - Set num_workers=0 to avoid potential multiprocessing issues on macOS\n",
    "# - Performance is still excellent on Apple Silicon despite these limitations\n",
    "#\n",
    "# Usage:\n",
    "# 1. Basic training: model, history = run_pipeline(CONFIG)\n",
    "# 2. Custom config: CONFIG.model_type = 'resnet50'; CONFIG.batch_size = 32\n",
    "# 3. Inference: embedding = model(image_tensor) with torch.no_grad()\n",
    "\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "########################################\n",
    "# CONFIGURATION (with type hints)\n",
    "########################################\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration for Loc2Vec training\"\"\"\n",
    "    # Model\n",
    "    model_type: str = 'cnn'  # 'cnn', 'resnet18', 'resnet50', 'densenet121'\n",
    "    use_pretrained: bool = True\n",
    "    freeze_backbone: bool = False\n",
    "    embedding_dim: int = 16  # Paper uses 16D\n",
    "    image_size: int = 128   # Paper uses 128x128\n",
    "\n",
    "    # Triplet loss\n",
    "    margin: float = 1.0\n",
    "    distance_threshold_km: float = 1.0  # 1km threshold for positive samples\n",
    "    use_softpn_loss: bool = True\n",
    "\n",
    "    # Training\n",
    "    batch_size: int = 20  # Paper: 20 locations per batch\n",
    "    pairs_per_location: int = 5  # Paper: 5 positive pairs per location\n",
    "    num_epochs: int = 1\n",
    "    learning_rate: float = 1e-3\n",
    "    weight_decay: float = 1e-4\n",
    "    patience: int = 15\n",
    "    use_amp: bool = True  # Mixed precision training (disabled for MPS)\n",
    "    gradient_clip: float = 1.0\n",
    "\n",
    "    # Data\n",
    "    data_file: str = 'tiles/tiles.csv'\n",
    "    train_ratio: float = 0.8\n",
    "    val_ratio: float = 0.1\n",
    "    random_seed: int = 42\n",
    "    num_workers: int = 0  # Set to 0 for debugging, increase for performance\n",
    "\n",
    "    # Kyiv-specific settings\n",
    "    kyiv_center_lat: float = 50.4501  # Kyiv center coordinates\n",
    "    kyiv_center_lon: float = 30.5234\n",
    "    coordinate_normalize: bool = True  # Normalize coordinates for better learning\n",
    "\n",
    "    # Output\n",
    "    save_model: bool = True\n",
    "    model_save_path: str = 'checkpoints/loc2vec_model.pth'\n",
    "    plot_training: bool = True\n",
    "\n",
    "CONFIG = Config()\n",
    "\n",
    "########################################\n",
    "# DEVICE AND SEED SETUP\n",
    "########################################\n",
    "\n",
    "def setup_environment(config: Config) -> torch.device:\n",
    "    \"\"\"Setup device and reproducibility\"\"\"\n",
    "    # Device selection with better handling\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        torch.backends.cudnn.benchmark = True  # Enable cudNN autotuner\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "        print(\"Using Apple Silicon GPU (MPS)\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(\"Using CPU\")\n",
    "\n",
    "    # Reproducibility\n",
    "    torch.manual_seed(config.random_seed)\n",
    "    np.random.seed(config.random_seed)\n",
    "    random.seed(config.random_seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(config.random_seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # Create directories\n",
    "    os.makedirs('checkpoints', exist_ok=True)\n",
    "    os.makedirs('plots', exist_ok=True)\n",
    "\n",
    "    return device\n",
    "\n",
    "device = setup_environment(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8666c875f31cf436",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T13:53:09.369200Z",
     "start_time": "2025-06-09T13:53:09.364966Z"
    }
   },
   "outputs": [],
   "source": [
    "class Loc2VecCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified CNN following the paper architecture:\n",
    "    - Conv layers with increasing filters (32->64->128->256->512)\n",
    "    - Batch norm and dropout\n",
    "    - Global average pooling\n",
    "    - FC layers to embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim: int = 16):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolutional backbone\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            # Block 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            # Block 4\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            # Block 5\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),  # Global average pooling\n",
    "        )\n",
    "\n",
    "        # Embedding head\n",
    "        self.embedder = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, embedding_dim)\n",
    "        )\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights using Kaiming initialization\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.embedder(x)\n",
    "        return F.normalize(x, p=2, dim=1)  # L2 normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9ae4b6146cdd930",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T13:53:09.384707Z",
     "start_time": "2025-06-09T13:53:09.380689Z"
    }
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# TRANSFER LEARNING MODELS\n",
    "########################################\n",
    "\n",
    "class Loc2VecTransferLearning(nn.Module):\n",
    "    \"\"\"Transfer learning with modern pretrained models\"\"\"\n",
    "\n",
    "    def __init__(self, model_type: str = 'resnet18', pretrained: bool = True,\n",
    "                 embedding_dim: int = 16, freeze_backbone: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        # Select backbone with proper weight handling for PyTorch 2.0+\n",
    "        if model_type == 'resnet18':\n",
    "            weights = models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "            self.backbone = models.resnet18(weights=weights)\n",
    "            num_features = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "\n",
    "        elif model_type == 'resnet50':\n",
    "            weights = models.ResNet50_Weights.IMAGENET1K_V2 if pretrained else None\n",
    "            self.backbone = models.resnet50(weights=weights)\n",
    "            num_features = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "\n",
    "        elif model_type == 'densenet121':\n",
    "            weights = models.DenseNet121_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "            self.backbone = models.densenet121(weights=weights)\n",
    "            num_features = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "        # Freeze backbone if requested\n",
    "        if freeze_backbone:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # Embedding head\n",
    "        self.embedder = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        features = self.backbone(x)\n",
    "        embeddings = self.embedder(features)\n",
    "        return F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "    def unfreeze_backbone(self, unfreeze_ratio: float = 1.0):\n",
    "        \"\"\"Gradually unfreeze backbone layers\"\"\"\n",
    "        layers = list(self.backbone.children())\n",
    "        num_to_unfreeze = int(len(layers) * unfreeze_ratio)\n",
    "        for layer in layers[-num_to_unfreeze:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5d1290b5beeceff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T13:53:09.403179Z",
     "start_time": "2025-06-09T13:53:09.397238Z"
    }
   },
   "outputs": [],
   "source": [
    "class TripletDataset(Dataset):\n",
    "    \"\"\"Dataset with proper coordinate handling for Kyiv\"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, transform=None, config: Config = CONFIG):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.config = config\n",
    "\n",
    "        # Normalize coordinates if using Kyiv data\n",
    "        if config.coordinate_normalize:\n",
    "            self.coords = self._normalize_coordinates(df[['x', 'y']].values)\n",
    "        else:\n",
    "            self.coords = df[['x', 'y']].values\n",
    "\n",
    "        # Convert distance threshold from km to normalized units\n",
    "        self.pos_threshold = self._km_to_normalized(config.distance_threshold_km)\n",
    "        self.neg_threshold = self.pos_threshold * 5  # Negative samples are 5x farther\n",
    "\n",
    "        # Build spatial index\n",
    "        self.spatial_index = NearestNeighbors(\n",
    "            n_neighbors=min(100, len(self.df)),\n",
    "            metric='euclidean',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        self.spatial_index.fit(self.coords)\n",
    "\n",
    "        print(f\"Dataset: {len(self.df)} samples\")\n",
    "        print(f\"Positive threshold: {self.pos_threshold:.4f} (normalized)\")\n",
    "\n",
    "    def _normalize_coordinates(self, coords: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Normalize coordinates relative to Kyiv center\"\"\"\n",
    "        # Center coordinates\n",
    "        centered = coords - np.array([self.config.kyiv_center_lon, self.config.kyiv_center_lat])\n",
    "        # Scale to roughly unit variance\n",
    "        scaled = centered / np.std(centered, axis=0)\n",
    "        return scaled\n",
    "\n",
    "    def _km_to_normalized(self, km: float) -> float:\n",
    "        \"\"\"Convert km to normalized coordinate units (approximate)\"\"\"\n",
    "        # Rough conversion: 1 degree â‰ˆ 111 km at equator\n",
    "        # For Kyiv latitude: adjust for latitude\n",
    "        lat_factor = np.cos(np.radians(self.config.kyiv_center_lat))\n",
    "        degrees_per_km = 1.0 / (111.0 * lat_factor)\n",
    "\n",
    "        if self.config.coordinate_normalize:\n",
    "            # Account for normalization scaling\n",
    "            std_lon = np.std(self.df['x'].values - self.config.kyiv_center_lon)\n",
    "            return km * degrees_per_km / std_lon\n",
    "        else:\n",
    "            return km * degrees_per_km\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def _load_image(self, idx: int) -> torch.Tensor:\n",
    "        \"\"\"Load image with error handling\"\"\"\n",
    "        try:\n",
    "            img_path = self.df.iloc[idx]['path']\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {idx}: {e}\")\n",
    "            image = Image.new('RGB', (self.config.image_size, self.config.image_size))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Get triplet following paper's strategy\"\"\"\n",
    "        anchor_img = self._load_image(idx)\n",
    "\n",
    "        # Find neighbors\n",
    "        distances, indices = self.spatial_index.kneighbors([self.coords[idx]], n_neighbors=50)\n",
    "        distances = distances[0]\n",
    "        indices = indices[0]\n",
    "\n",
    "        # Positive: nearby location (not self)\n",
    "        pos_mask = (distances > 0) & (distances < self.pos_threshold)\n",
    "        pos_candidates = indices[pos_mask]\n",
    "\n",
    "        if len(pos_candidates) == 0:\n",
    "            # Fallback: use closest non-self\n",
    "            pos_candidates = indices[1:6]\n",
    "\n",
    "        pos_idx = np.random.choice(pos_candidates)\n",
    "\n",
    "        # Negative: far location\n",
    "        neg_mask = distances > self.neg_threshold\n",
    "        neg_candidates = indices[neg_mask]\n",
    "\n",
    "        if len(neg_candidates) == 0:\n",
    "            # Fallback: use farthest available\n",
    "            neg_candidates = indices[-10:]\n",
    "\n",
    "        neg_idx = np.random.choice(neg_candidates)\n",
    "\n",
    "        return anchor_img, self._load_image(pos_idx), self._load_image(neg_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c054a1216af9d73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T13:53:09.419003Z",
     "start_time": "2025-06-09T13:53:09.415813Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "########################################\n",
    "# TRIPLET LOSS VARIANTS\n",
    "########################################\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"Standard triplet loss with margin\"\"\"\n",
    "\n",
    "    def __init__(self, margin: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor: torch.Tensor, positive: torch.Tensor,\n",
    "                negative: torch.Tensor) -> torch.Tensor:\n",
    "        pos_dist = F.pairwise_distance(anchor, positive, p=2)\n",
    "        neg_dist = F.pairwise_distance(anchor, negative, p=2)\n",
    "\n",
    "        loss = F.relu(pos_dist - neg_dist + self.margin)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class SoftPNTripletLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Soft Positive/Negative mining triplet loss (simplified version)\n",
    "    Paper's key contribution - uses soft assignments\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin: float = 1.0, temperature: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, anchor: torch.Tensor, positive: torch.Tensor,\n",
    "                negative: torch.Tensor) -> torch.Tensor:\n",
    "        # Standard triplet loss for now\n",
    "        # Full SoftPN implementation would require batch-wise mining\n",
    "        pos_dist = F.pairwise_distance(anchor, positive, p=2)\n",
    "        neg_dist = F.pairwise_distance(anchor, negative, p=2)\n",
    "\n",
    "        # Soft margin using log-sum-exp\n",
    "        loss = torch.log1p(torch.exp((pos_dist - neg_dist) / self.temperature))\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bb39ed52ec0f79e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T13:53:09.438071Z",
     "start_time": "2025-06-09T13:53:09.431095Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_transforms(config: Config) -> Tuple[transforms.Compose, transforms.Compose]:\n",
    "    \"\"\"Create augmentation transforms\"\"\"\n",
    "    # Training augmentations (following paper)\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((config.image_size, config.image_size)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=45),  # Paper mentions rotation\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Validation/test transforms\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((config.image_size, config.image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    return train_transform, val_transform\n",
    "\n",
    "########################################\n",
    "# TRAINING WITH MODERN FEATURES\n",
    "########################################\n",
    "\n",
    "def train_epoch(model: nn.Module, loader: DataLoader, criterion: nn.Module,\n",
    "                optimizer: optim.Optimizer, device: torch.device,\n",
    "                config: Config) -> float:\n",
    "    \"\"\"Training epoch with mixed precision support\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    pbar = tqdm.tqdm(loader, desc=\"Training\")\n",
    "    for batch_idx, (anchors, positives, negatives) in enumerate(pbar):\n",
    "        anchors = anchors.to(device, non_blocking=True)\n",
    "        positives = positives.to(device, non_blocking=True)\n",
    "        negatives = negatives.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)  # More efficient than zero_grad()\n",
    "\n",
    "        anchor_emb = model(anchors)\n",
    "        positive_emb = model(positives)\n",
    "        negative_emb = model(negatives)\n",
    "        loss = criterion(anchor_emb, positive_emb, negative_emb)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        if config.gradient_clip > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_epoch(model: nn.Module, loader: DataLoader, criterion: nn.Module,\n",
    "                   device: torch.device) -> Tuple[float, torch.Tensor]:\n",
    "    \"\"\"Validation epoch\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_embeddings = []\n",
    "\n",
    "    for anchors, positives, negatives in tqdm.tqdm(loader, desc=\"Validation\"):\n",
    "        anchors = anchors.to(device, non_blocking=True)\n",
    "        positives = positives.to(device, non_blocking=True)\n",
    "        negatives = negatives.to(device, non_blocking=True)\n",
    "\n",
    "        anchor_emb = model(anchors)\n",
    "        positive_emb = model(positives)\n",
    "        negative_emb = model(negatives)\n",
    "\n",
    "        loss = criterion(anchor_emb, positive_emb, negative_emb)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        all_embeddings.append(anchor_emb.cpu())\n",
    "\n",
    "    all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "    return total_loss / len(loader), all_embeddings\n",
    "\n",
    "########################################\n",
    "# MAIN TRAINING FUNCTION\n",
    "########################################\n",
    "\n",
    "def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader,\n",
    "                config: Config, device: torch.device) -> Dict:\n",
    "    \"\"\"Complete training loop with modern features\"\"\"\n",
    "    criterion = SoftPNTripletLoss(margin=config.margin) if config.use_softpn_loss else TripletLoss(margin=config.margin)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "\n",
    "    # Training state\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{config.num_epochs}\")\n",
    "\n",
    "        # Train\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device, config)\n",
    "        history['train_loss'].append(train_loss)\n",
    "\n",
    "        # Validate\n",
    "        val_loss, _ = validate_epoch(model, val_loader, criterion, device)\n",
    "        history['val_loss'].append(val_loss)\n",
    "\n",
    "        # Update scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= config.patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(best_state)\n",
    "    history['best_val_loss'] = best_val_loss\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T13:53:10.024194Z",
     "start_time": "2025-06-09T13:53:09.449209Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Running on Apple Silicon (MPS)\n",
      "Optimizations applied:\n",
      "- torch.compile disabled (not supported on MPS)\n",
      "- Mixed precision disabled (not supported on MPS)\n",
      "- num_workers set to 0 (recommended for macOS)\n",
      "==================================================\n",
      "\n",
      "Starting Loc2Vec Training Pipeline\n",
      "==================================================\n",
      "\n",
      "1. Loading data...\n",
      "Found 16790 valid tiles\n",
      "Train: 13432 | Val: 1679 | Test: 1679\n",
      "\n",
      "2. Creating data loaders...\n",
      "Dataset: 13432 samples\n",
      "Positive threshold: 0.1220 (normalized)\n",
      "Dataset: 1679 samples\n",
      "Positive threshold: 0.1237 (normalized)\n",
      "\n",
      "3. Creating cnn model...\n",
      "Total params: 1,638,288 | Trainable: 1,638,288\n",
      "\n",
      "4. Training model...\n",
      "\n",
      "Epoch 1/2\n",
      "\n",
      "âŒ Error during training: 'module' object is not callable. Did you mean: 'tqdm.tqdm(...)'?\n",
      "\n",
      "Troubleshooting tips:\n",
      "1. Ensure tiles.csv and image files exist\n",
      "2. Check file paths in the CSV\n",
      "3. Try reducing batch_size if out of memory\n",
      "4. Set num_workers=0 if multiprocessing issues\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable. Did you mean: 'tqdm.tqdm(...)'?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 177\u001b[39m\n\u001b[32m    174\u001b[39m test_config = Config(num_epochs=\u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# Quick test run\u001b[39;00m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     model, history, df_full = \u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m     \u001b[38;5;66;03m# Plot training curves\u001b[39;00m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m test_config.plot_training:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 141\u001b[39m, in \u001b[36mrun_pipeline\u001b[39m\u001b[34m(config)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m    140\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m4. Training model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m history = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[38;5;66;03m# Save model\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.save_model:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 107\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, config, device)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.num_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m history[\u001b[33m'\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m'\u001b[39m].append(train_loss)\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# Validate\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, loader, criterion, optimizer, device, config)\u001b[39m\n\u001b[32m     31\u001b[39m model.train()\n\u001b[32m     32\u001b[39m total_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m pbar = \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTraining\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (anchors, positives, negatives) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pbar):\n\u001b[32m     36\u001b[39m     anchors = anchors.to(device, non_blocking=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mTypeError\u001b[39m: 'module' object is not callable. Did you mean: 'tqdm.tqdm(...)'?"
     ]
    }
   ],
   "source": [
    "\n",
    "########################################\n",
    "# INFERENCE UTILITIES\n",
    "########################################\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_embedding(model: nn.Module, image_path: str, transform: transforms.Compose,\n",
    "                  device: torch.device) -> np.ndarray:\n",
    "    \"\"\"Get embedding for a single image\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Load and transform image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Get embedding\n",
    "    embedding = model(image_tensor)\n",
    "    return embedding.cpu().numpy().squeeze()\n",
    "\n",
    "\n",
    "def find_similar_tiles(query_embedding: np.ndarray, embeddings_db: np.ndarray,\n",
    "                       coords_db: np.ndarray, top_k: int = 5) -> List[Dict]:\n",
    "    \"\"\"Find most similar tiles to a query embedding\"\"\"\n",
    "    # Compute cosine similarities (embeddings are L2 normalized)\n",
    "    similarities = embeddings_db @ query_embedding\n",
    "\n",
    "    # Get top-k indices\n",
    "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            'index': idx,\n",
    "            'similarity': similarities[idx],\n",
    "            'coordinates': coords_db[idx],\n",
    "            'distance_km': np.linalg.norm(coords_db[idx] - coords_db[0]) * 111  # Rough km conversion\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def create_embedding_database(model: nn.Module, df: pd.DataFrame,\n",
    "                            transform: transforms.Compose, device: torch.device,\n",
    "                            batch_size: int = 32) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Create database of embeddings for all tiles\"\"\"\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    coords = []\n",
    "\n",
    "    # Create temporary dataset\n",
    "    dataset = TripletDataset(df, transform=transform, config=CONFIG)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    print(\"Creating embedding database...\")\n",
    "    with torch.no_grad():\n",
    "        for i, (anchors, _, _) in enumerate(tqdm(loader)):\n",
    "            anchors = anchors.to(device)\n",
    "            emb = model(anchors)\n",
    "            embeddings.append(emb.cpu().numpy())\n",
    "\n",
    "            # Get corresponding coordinates\n",
    "            batch_coords = df.iloc[i*batch_size:(i+1)*batch_size][['x', 'y']].values\n",
    "            coords.append(batch_coords)\n",
    "\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    coords = np.vstack(coords)\n",
    "\n",
    "    return embeddings, coords\n",
    "\n",
    "\n",
    "########################################\n",
    "# COMPLETE PIPELINE\n",
    "########################################\n",
    "\n",
    "def run_pipeline(config: Config = CONFIG) -> Tuple[nn.Module, Dict, pd.DataFrame]:\n",
    "    \"\"\"Run complete training pipeline\"\"\"\n",
    "    print(\"Starting Loc2Vec Training Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Load data\n",
    "    print(\"\\n1. Loading data...\")\n",
    "    df = pd.read_csv(config.data_file)\n",
    "    df_full = df[df['service'] == 'full'].copy()\n",
    "\n",
    "    # Check file existence\n",
    "    df_full['exists'] = df_full['path'].apply(lambda x: Path(x).exists())\n",
    "    df_full = df_full[df_full['exists']].drop(columns=['exists'])\n",
    "    print(f\"Found {len(df_full)} valid tiles\")\n",
    "\n",
    "    # Split data\n",
    "    train_df, temp_df = train_test_split(df_full, test_size=1-config.train_ratio, random_state=config.random_seed)\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=config.random_seed)\n",
    "\n",
    "    print(f\"Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")\n",
    "\n",
    "    # Create datasets and loaders\n",
    "    print(\"\\n2. Creating data loaders...\")\n",
    "    train_transform, val_transform = create_transforms(config)\n",
    "\n",
    "    train_dataset = TripletDataset(train_df, transform=train_transform, config=config)\n",
    "    val_dataset = TripletDataset(val_df, transform=val_transform, config=config)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=config.num_workers,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True if config.num_workers > 0 else False\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.num_workers,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True if config.num_workers > 0 else False\n",
    "    )\n",
    "\n",
    "    # Create model\n",
    "    print(f\"\\n3. Creating {config.model_type} model...\")\n",
    "    if config.model_type == 'cnn':\n",
    "        model = Loc2VecCNN(embedding_dim=config.embedding_dim)\n",
    "    else:\n",
    "        model = Loc2VecTransferLearning(\n",
    "            model_type=config.model_type,\n",
    "            pretrained=config.use_pretrained,\n",
    "            embedding_dim=config.embedding_dim,\n",
    "            freeze_backbone=config.freeze_backbone\n",
    "        )\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Print model info\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total params: {total_params:,} | Trainable: {trainable_params:,}\")\n",
    "\n",
    "    # Train model\n",
    "    print(\"\\n4. Training model...\")\n",
    "    history = train_model(model, train_loader, val_loader, config, device)\n",
    "\n",
    "    # Save model\n",
    "    if config.save_model:\n",
    "        print(\"\\n5. Saving model...\")\n",
    "        save_path = Path(config.model_save_path)\n",
    "        save_path.parent.mkdir(exist_ok=True)\n",
    "\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'config': config,\n",
    "            'history': history,\n",
    "            'pytorch_version': torch.__version__,\n",
    "        }, save_path)\n",
    "\n",
    "        print(f\"Model saved to {save_path}\")\n",
    "\n",
    "    return model, history, df_full\n",
    "\n",
    "\n",
    "# Run the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    # Quick test for MPS compatibility\n",
    "    if device.type == 'mps':\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Running on Apple Silicon (MPS)\")\n",
    "        print(\"Optimizations applied:\")\n",
    "        print(\"- torch.compile disabled (not supported on MPS)\")\n",
    "        print(\"- Mixed precision disabled (not supported on MPS)\")\n",
    "        print(\"- num_workers set to 0 (recommended for macOS)\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "    # Run with reduced epochs for testing\n",
    "    test_config = Config(num_epochs=2)  # Quick test run\n",
    "\n",
    "    try:\n",
    "        model, history, df_full = run_pipeline(test_config)\n",
    "\n",
    "        # Plot training curves\n",
    "        if test_config.plot_training:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "            plt.plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Triplet Loss')\n",
    "            plt.title('Loc2Vec Training Progress')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "\n",
    "            plot_path = Path('plots/training_curves.png')\n",
    "            plot_path.parent.mkdir(exist_ok=True)\n",
    "            plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"\\nTraining curves saved to {plot_path}\")\n",
    "            plt.show()\n",
    "\n",
    "        print(\"\\nâœ… Training completed successfully!\")\n",
    "        print(f\"Best validation loss: {history['best_val_loss']:.4f}\")\n",
    "\n",
    "        # Example: How to use the trained model\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"EXAMPLE: Using the trained model\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # Load validation transform\n",
    "        _, val_transform = create_transforms(test_config)\n",
    "\n",
    "        # Example 1: Get embedding for a single image\n",
    "        print(\"\\n1. Getting embedding for a single tile:\")\n",
    "        sample_path = df_full.iloc[0]['path']\n",
    "        if Path(sample_path).exists():\n",
    "            embedding = get_embedding(model, sample_path, val_transform, device)\n",
    "            print(f\"   Embedding shape: {embedding.shape}\")\n",
    "            print(f\"   Embedding sample: {embedding[:5]}...\")\n",
    "\n",
    "        # Example 2: Find similar tiles\n",
    "        print(\"\\n2. Finding similar tiles:\")\n",
    "        print(\"   (This would require building an embedding database first)\")\n",
    "        print(\"   embeddings_db, coords_db = create_embedding_database(model, df, transform, device)\")\n",
    "        print(\"   similar = find_similar_tiles(query_embedding, embeddings_db, coords_db)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error during training: {e}\")\n",
    "        print(\"\\nTroubleshooting tips:\")\n",
    "        print(\"1. Ensure tiles.csv and image files exist\")\n",
    "        print(\"2. Check file paths in the CSV\")\n",
    "        print(\"3. Try reducing batch_size if out of memory\")\n",
    "        print(\"4. Set num_workers=0 if multiprocessing issues\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb5050b33de50e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# LEGACY VISUALIZATION CODE (REFERENCE ONLY)\n",
    "# ========================================\n",
    "\n",
    "# Note: This cell contains the original standalone visualization code.\n",
    "# The actual visualization functions are now integrated in the next cell.\n",
    "# You can run this cell for reference, but the main workflow uses the \n",
    "# integrated functions in the next cell.\n",
    "\n",
    "print(\"ðŸ“š Legacy visualization code loaded (reference only)\")\n",
    "print(\"ðŸŽ¨ Main visualization functions are in the next cell\")\n",
    "print(\"âœ¨ Use create_embeddings_visualization() after training completes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dc6efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# EMBEDDING VISUALIZATION FOR TENSORBOARD\n",
    "# ========================================\n",
    "\n",
    "# Note: SummaryWriter import is needed for TensorBoard visualization\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    \"\"\"Dataset for inference - loads images with metadata\"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, transform=None, config: Config = None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.config = config or Config()\n",
    "\n",
    "        # Normalize coordinates if needed\n",
    "        if self.config.coordinate_normalize:\n",
    "            self.coords = self._normalize_coordinates(df[['x', 'y']].values)\n",
    "        else:\n",
    "            self.coords = df[['x', 'y']].values\n",
    "\n",
    "    def _normalize_coordinates(self, coords: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Normalize coordinates relative to Kyiv center\"\"\"\n",
    "        centered = coords - np.array([self.config.kyiv_center_lon, self.config.kyiv_center_lat])\n",
    "        scaled = centered / np.std(centered, axis=0)\n",
    "        return scaled\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = row['path']\n",
    "\n",
    "        # Load image\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            image = Image.new('RGB', (self.config.image_size, self.config.image_size), color='red')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Return image and metadata\n",
    "        metadata = {\n",
    "            'coordinates': [row['x'], row['y']],\n",
    "            'normalized_coords': self.coords[idx].tolist() if hasattr(self, 'coords') else [row['x'], row['y']],\n",
    "            'zoom': row.get('zoom', 0),\n",
    "            'path': img_path,\n",
    "            'index': idx\n",
    "        }\n",
    "\n",
    "        return image, metadata\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle metadata dictionaries\"\"\"\n",
    "    images = torch.stack([item[0] for item in batch])\n",
    "    metadata_list = [item[1] for item in batch]  # Keep as list of dicts\n",
    "    return images, metadata_list\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_embeddings_and_thumbnails(\n",
    "    model: nn.Module,\n",
    "    dataset: InferenceDataset,\n",
    "    config: Config,\n",
    "    device: torch.device,\n",
    "    max_tiles: int = 2000\n",
    ") -> Tuple[torch.Tensor, List[List[str]], torch.Tensor, List[dict], List[str]]:\n",
    "    \"\"\"Generate embeddings, metadata, and thumbnails for projector\"\"\"\n",
    "    \n",
    "    loader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=config.batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=0,  # Use 0 for macOS compatibility\n",
    "        pin_memory=True if device.type != 'cpu' else False,\n",
    "        collate_fn=custom_collate_fn  # Use custom collate function\n",
    "    )\n",
    "\n",
    "    all_embeddings = []\n",
    "    all_metadata = []\n",
    "    all_thumbnails = []\n",
    "    all_metadata_dicts = []\n",
    "\n",
    "    # Thumbnail transform for projector\n",
    "    thumbnail_transform = transforms.Compose([\n",
    "        transforms.Resize((32, 32)),  # Small thumbnails\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    print(f\"Generating embeddings for visualization...\")\n",
    "    \n",
    "    total_processed = 0\n",
    "    for batch_idx, (images, metadata_batch) in enumerate(tqdm.tqdm(loader, desc=\"Processing tiles\")):\n",
    "        if total_processed >= max_tiles:\n",
    "            break\n",
    "            \n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = model(images)\n",
    "        all_embeddings.append(embeddings.cpu())\n",
    "        \n",
    "        # Process metadata and create thumbnails\n",
    "        for i, metadata in enumerate(metadata_batch):\n",
    "            if total_processed >= max_tiles:\n",
    "                break\n",
    "                \n",
    "            # Format metadata for TensorBoard\n",
    "            coords = metadata['coordinates']\n",
    "            norm_coords = metadata['normalized_coords']\n",
    "            zoom = metadata['zoom']\n",
    "            idx = metadata['index']\n",
    "            \n",
    "            # Create metadata list (not string) for TensorBoard\n",
    "            metadata_list = [\n",
    "                f\"{coords[0]:.6f}\",\n",
    "                f\"{coords[1]:.6f}\", \n",
    "                f\"{norm_coords[0]:.4f}\",\n",
    "                f\"{norm_coords[1]:.4f}\",\n",
    "                str(zoom),\n",
    "                str(idx)\n",
    "            ]\n",
    "            all_metadata.append(metadata_list)\n",
    "            all_metadata_dicts.append(metadata)\n",
    "            \n",
    "            # Create thumbnail from original image\n",
    "            # Note: We need to denormalize the image first\n",
    "            img_tensor = images[i].cpu()\n",
    "            \n",
    "            # Denormalize\n",
    "            mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "            std = torch.tensor([0.229, 0.224, 0.225])\n",
    "            img_tensor = img_tensor * std.view(3, 1, 1) + mean.view(3, 1, 1)\n",
    "            img_tensor = torch.clamp(img_tensor, 0, 1)\n",
    "            \n",
    "            # Create thumbnail\n",
    "            thumbnail = thumbnail_transform(transforms.ToPILImage()(img_tensor))\n",
    "            all_thumbnails.append(thumbnail)\n",
    "            \n",
    "            total_processed += 1\n",
    "\n",
    "    # Combine all data\n",
    "    embeddings_tensor = torch.cat(all_embeddings, dim=0)[:max_tiles]\n",
    "    thumbnails_tensor = torch.stack(all_thumbnails)[:max_tiles]\n",
    "    \n",
    "    # Define header for metadata  \n",
    "    metadata_header = [\"lon\", \"lat\", \"norm_lon\", \"norm_lat\", \"zoom\", \"index\"]\n",
    "\n",
    "    print(f\"âœ… Generated {len(embeddings_tensor)} embeddings\")\n",
    "    \n",
    "    return embeddings_tensor, all_metadata[:max_tiles], thumbnails_tensor, all_metadata_dicts[:max_tiles], metadata_header\n",
    "\n",
    "\n",
    "def create_projector_visualization(\n",
    "    embeddings: torch.Tensor,\n",
    "    metadata: List[List[str]],\n",
    "    thumbnails: torch.Tensor,\n",
    "    metadata_dicts: List[dict],\n",
    "    metadata_header: List[str],\n",
    "    log_dir: str = \"runs/loc2vec_projector\"\n",
    "):\n",
    "    \"\"\"Create TensorBoard projector visualization\"\"\"\n",
    "    \n",
    "    print(f\"Creating TensorBoard projector in {log_dir}...\")\n",
    "    \n",
    "    # Create log directory\n",
    "    Path(log_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Initialize TensorBoard writer\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    \n",
    "    # Add embeddings with metadata and thumbnails\n",
    "    writer.add_embedding(\n",
    "        mat=embeddings,\n",
    "        metadata=metadata,\n",
    "        label_img=thumbnails,\n",
    "        tag=\"Loc2Vec_Embeddings\",\n",
    "        metadata_header=metadata_header\n",
    "    )\n",
    "    \n",
    "    # Add some statistics\n",
    "    writer.add_text(\"Dataset_Info\", f\"\"\"\n",
    "    ## Loc2Vec Embedding Visualization\n",
    "    \n",
    "    - **Total tiles**: {len(embeddings)}\n",
    "    - **Embedding dimension**: {embeddings.shape[1]}\n",
    "    - **Coordinate range**: \n",
    "      - Longitude: {min(m['coordinates'][0] for m in metadata_dicts):.4f} to {max(m['coordinates'][0] for m in metadata_dicts):.4f}\n",
    "      - Latitude: {min(m['coordinates'][1] for m in metadata_dicts):.4f} to {max(m['coordinates'][1] for m in metadata_dicts):.4f}\n",
    "    \n",
    "    ## How to use:\n",
    "    1. Select the \"Loc2Vec_Embeddings\" in the projector\n",
    "    2. Try different projection methods (PCA, t-SNE, UMAP)\n",
    "    3. Color points by metadata (coordinates, zoom level)\n",
    "    4. Hover over points to see thumbnail images\n",
    "    5. Search for specific tiles by index\n",
    "    \"\"\")\n",
    "    \n",
    "    writer.close()\n",
    "    \n",
    "    print(\"âœ… Projector visualization created!\")\n",
    "    print(f\"\\nðŸš€ To view the visualization, run:\")\n",
    "    print(f\"   ./run_visualization.sh\")\n",
    "    print(f\"   OR manually: tensorboard --logdir=runs\")\n",
    "    print(f\"\\n   Then open your browser to http://localhost:6006\")\n",
    "    print(f\"   Navigate to the 'PROJECTOR' tab\")\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# CREATE VISUALIZATION AFTER TRAINING\n",
    "# ========================================\n",
    "\n",
    "def create_embeddings_visualization(model, df_data, config, device, max_tiles=2000):\n",
    "    \"\"\"Create embeddings visualization for trained model\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸŽ¨ CREATING TENSORBOARD EMBEDDINGS VISUALIZATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Filter valid tiles\n",
    "    df_vis = df_data[df_data['service'] == 'full'].copy()\n",
    "    df_vis['exists'] = df_vis['path'].apply(lambda x: Path(x).exists())\n",
    "    df_valid = df_vis[df_vis['exists']].drop(columns=['exists'])\n",
    "    \n",
    "    print(f\"Found {len(df_valid)} valid tiles for visualization\")\n",
    "    \n",
    "    # Limit for performance\n",
    "    if len(df_valid) > max_tiles:\n",
    "        print(f\"Limiting to {max_tiles} tiles for performance\")\n",
    "        df_valid = df_valid.sample(n=max_tiles, random_state=config.random_seed)\n",
    "    \n",
    "    # Create dataset\n",
    "    _, val_transform = create_transforms(config)\n",
    "    dataset = InferenceDataset(df_valid, transform=val_transform, config=config)\n",
    "    \n",
    "    # Generate embeddings and thumbnails\n",
    "    embeddings, metadata, thumbnails, metadata_dicts, metadata_header = generate_embeddings_and_thumbnails(\n",
    "        model, dataset, config, device, max_tiles\n",
    "    )\n",
    "    \n",
    "    # Create projector visualization\n",
    "    create_projector_visualization(embeddings, metadata, thumbnails, metadata_dicts, metadata_header)\n",
    "    \n",
    "    return embeddings, metadata_dicts\n",
    "\n",
    "print(\"ðŸŽ¨ Embedding visualization functions loaded!\")\n",
    "print(\"Use create_embeddings_visualization(model, df, config, device) to generate TensorBoard projector data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5ffa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# GENERATE EMBEDDINGS VISUALIZATION\n",
    "# ========================================\n",
    "\n",
    "# After training is complete, generate TensorBoard visualization\n",
    "if 'model' in locals() and 'df_full' in locals():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸŽ¨ GENERATING EMBEDDINGS VISUALIZATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # Create embeddings visualization\n",
    "        embeddings, metadata_dicts = create_embeddings_visualization(\n",
    "            model=model,\n",
    "            df_data=df_full, \n",
    "            config=CONFIG,\n",
    "            device=device,\n",
    "            max_tiles=2000  # Adjust this number based on your needs\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nâœ… Visualization created with {len(embeddings)} tile embeddings!\")\n",
    "        print(\"\\nðŸŽ¯ Next steps:\")\n",
    "        print(\"1. Run: ./run_visualization.sh\")\n",
    "        print(\"2. Open http://localhost:6006 in your browser\") \n",
    "        print(\"3. Navigate to the PROJECTOR tab\")\n",
    "        print(\"4. Explore your Loc2Vec embeddings!\")\n",
    "        \n",
    "        print(\"\\nðŸ” What to look for:\")\n",
    "        print(\"- Geographic clustering: Similar locations should cluster together\")\n",
    "        print(\"- Urban patterns: Different development types should separate\")\n",
    "        print(\"- Smooth transitions: Neighboring areas should have similar embeddings\")\n",
    "        print(\"- Scale consistency: Similar zoom levels should cluster\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error creating visualization: {e}\")\n",
    "        print(\"You can still create it manually later using:\")\n",
    "        print(\"create_embeddings_visualization(model, df_full, CONFIG, device)\")\n",
    "        \n",
    "else:\n",
    "    print(\"ðŸŽ¨ Visualization functions ready!\")\n",
    "    print(\"Train your model first, then run:\")\n",
    "    print(\"create_embeddings_visualization(model, df_full, CONFIG, device)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92f5c732aeef74c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
